{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div style=\"text-align:center; color:green\">n-gram feature Extraction and Model Building using TF-IDF</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## importing neccesary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: Qt5Agg\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from nltk import word_tokenize\n",
    "from pip._internal.utils.misc import tabulate\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import glob\n",
    "import pandas as pd\n",
    "%run stylometric_Features.ipynb import FeatureExtration\n",
    "import seaborn as sns\n",
    "import matplotlib as plt\n",
    "import warnings\n",
    "import string\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.svm import SVC\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_instances={\n",
    "\n",
    "'hrudayashiva':[\"01\",\"02\",\"03\",\"04\",\"05\",\"06\",\"07\",\"08\",\"09\",10,11,12],\n",
    "    'ravibeliger':[13,14,15,16,17,18,19,20,21,22,23,24,25],\n",
    "    'somashaker':[26,27,28,29,30,31,32,33,34,35,36],\n",
    "    'Other author':[]\n",
    "}\n",
    "#from features_NormalApproach import FeatureExtration\n",
    "\n",
    "\n",
    "stop_words_list = \"C://Users/RAVIKUMAR/PycharmProjects/Authorship_Attribution_Instance/Data_2/new_stop_words.txt\"\n",
    "data_folder = \"C://Users/RAVIKUMAR/PycharmProjects/Authorship_Attribution_Instance/Data_2/train\"\n",
    "instance_by_author={}\n",
    "for author,AllFiles_Per_author in all_instances.items():\n",
    "    for i in AllFiles_Per_author:\n",
    "        for name in glob.glob(f\"C://Users/RAVIKUMAR/PycharmProjects/Authorship_Attribution_Instance/Data_2/train/instance{i}.txt\"):\n",
    "            with open(name, encoding='utf-8') as file:\n",
    "                instance_by_author[author+str(i)]=file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatiser = WordNetLemmatizer()\n",
    "stopword=[]\n",
    "with open(stop_words_list, encoding='utf-8') as file:\n",
    "    reader=file.read()\n",
    "    stopword.append(reader)\n",
    "def text_process(tex):\n",
    "    nopunct = [char for char in tex if char not in string.punctuation]\n",
    "    nopunct = ''.join(nopunct)\n",
    "    return nopunct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngramFeatureExtraction():\n",
    "    all_authors=[]\n",
    "    text=[]\n",
    "    for author, file in instance_by_author.items():\n",
    "        all_authors.append(author)\n",
    "        text.append(file)\n",
    "    df=pd.DataFrame()\n",
    "    df['author']=all_authors\n",
    "#     df['author'][0:13]='hrudayashiva'\n",
    "#     df['author'][13:25]='ravibeligeri'\n",
    "#     df['author'][25:36]='somashaker'\n",
    "    Y = pd.Series(df[\"author\"])\n",
    "    Y.replace(r'(^ra.*)','ravibeligeri',regex=True, inplace = True)\n",
    "    Y.replace(r'(^hr.*)','hrudayashiva',regex=True, inplace = True)\n",
    "    Y.replace(r'(^som.*)','somashaker',regex=True, inplace = True)            \n",
    "    df['text']=text\n",
    "\n",
    "    y = Y\n",
    "    X = df['text']\n",
    "    \n",
    "    # 80-20 splitting the dataset (80%->Training and 20%->Validation)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.2, random_state=1234)\n",
    "    clean_data=[]\n",
    "    \n",
    "    #defining the bag-of-words transformer on the text-processed corpus # i.e., text_process() declared in II is executed...\n",
    "    for i in X_train:\n",
    "        clean_data.append(text_process(i))\n",
    "    X_train = clean_data\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    #bow_transformer=CountVectorizer(ngram_range=(2,3),min_df=5, analyzer=text_process).fit(X_train)\n",
    "    word_vector=TfidfVectorizer(analyzer=\"word\", ngram_range=(2,3),max_features=2000, binary=False).fit(X_train)\n",
    "    word_vector1=TfidfVectorizer(analyzer=\"word\", ngram_range=(3,4),max_features=2000, binary=False).fit(X_train)\n",
    "\n",
    "    char_vector=TfidfVectorizer(analyzer=\"char\", ngram_range=(2,3),max_features=2000, binary=False,min_df=3).fit(X_train)\n",
    "    char_vector1=TfidfVectorizer(analyzer=\"char\", ngram_range=(3,4),max_features=2000, binary=False,min_df=3).fit(X_train)\n",
    "    bow_transformer=FeatureUnion([ (\"chars\",char_vector),(\"word\",word_vector),(\"3_chars\",char_vector1),(\"3_word\",word_vector1)] )\n",
    "\n",
    "\n",
    "    #bow_transformer =  TfidfVectorizer(min_df=3, analyzer=text_process).fit(X_train)\n",
    "    #transforming into Bag-of-Words and hence textual data to numeric..\n",
    "    feature_names = bow_transformer.get_feature_names()\n",
    "    text_bow_train=bow_transformer.transform(X_train)#ONLY TRAINING DATA\n",
    "\n",
    "    # transforming into Bag-of-Words and hence textual data to numeric..\n",
    "    text_bow_test=bow_transformer.transform(X_test)#TEST DATA\n",
    "    \n",
    "    # instantiating the model with Multinomial Naive Bayes..\n",
    "    model =SVC(kernel='linear')\n",
    "    #model = model.fit(flatten_final.toarray(), y_train)\n",
    "\n",
    "    #model = GaussianNB()\n",
    "    #model= model.fit(flatten_final.toarray(), y_train)\n",
    "\n",
    "    #model = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "    #model = model.fit(flatten_final.toarray(), y_train)\n",
    "\n",
    "\n",
    "    # training the model...\n",
    "    model = model.fit(text_bow_train, y_train)\n",
    "    #import\n",
    "    print(\"Training Accuracy :\",model.score(text_bow_train, y_train))\n",
    "    print(\"Testing Acuuracy  : \",model.score(text_bow_test, y_test))\n",
    "\n",
    "    predictions = model.predict(text_bow_test)\n",
    "    pred = pd.DataFrame()\n",
    "    pred[\"text\"] = X_test\n",
    "    pred[\"author\"]=predictions\n",
    "    test_score = accuracy_score(y_test, predictions)\n",
    "    print(classification_report(y_test, predictions))\n",
    "#     print(pred,\"\\n\")\n",
    "#     print(df)\n",
    "    return pred,df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
