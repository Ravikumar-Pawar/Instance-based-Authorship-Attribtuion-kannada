{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instance-Based Authorship Attribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>importing necessary libraries </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io, os, sys, types\n",
    "import IPython\n",
    "%run stylometric_Features.ipynb import FeatureExtration\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import glob\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy import sparse\n",
    "import math\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import preprocessing\n",
    "import operator\n",
    "import itertools\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from stylometric_Features import LexicalFeatures,SyntacticFeatures\n",
    "#from stylometric_Features import FeatureExtration\n",
    "#from ngrams_Features import train_model\n",
    "\n",
    "#data reading mutilple files\n",
    "#hrudayashiva-instance-->01-12\n",
    "#ravi beligeri-isntance-->13-25\n",
    "#somashaker-isntance-->26-36"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## reading file and extracting Lexical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "all_instances={\n",
    "'hrudayashiva':[\"01\",\"02\",\"03\",\"04\",\"05\",\"06\",\"07\",\"08\",\"09\",10,11,12],\n",
    "    'ravibeliger':[13,14,15,16,17,18,19,20,21,22,23,24,25],\n",
    "    'somashaker':[26,27,28,29,30,31,32,33,34,35,36]\n",
    "}\n",
    "\n",
    "stop_words_list = \"C://Users/RAVIKUMAR/PycharmProjects/Authorship_Attribution_Instance/Data_2/new_stop_words.txt\"\n",
    "\n",
    "data_folder = \"C://Users/RAVIKUMAR/PycharmProjects/Authorship_Attribution_Instance/Data_2/train\"\n",
    "instance_by_author={}\n",
    "for author,AllFiles_Per_author in all_instances.items():\n",
    "    for i in AllFiles_Per_author:\n",
    "        for name in glob.glob(f\"C://Users/RAVIKUMAR/PycharmProjects/Authorship_Attribution_Instance/Data_2/train/instance{i}.txt\"):\n",
    "            with open(name, encoding='utf-8') as file:\n",
    "                instance_by_author[author+str(i)]=file.read()\n",
    "all_authors=[]\n",
    "text=[]\n",
    "original = pd.DataFrame()\n",
    "for author, file in instance_by_author.items():\n",
    "    all_authors.append(author)\n",
    "    text.append(file)\n",
    "original[\"Authors with id\"] = all_authors\n",
    "original[\"text files\"]=text\n",
    "df=pd.DataFrame()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labelencoder = LabelEncoder()\n",
    "# y = labelencoder.fit_transform(y)\n",
    "X=original[\"text files\"]\n",
    "y=all_authors\n",
    "# 80-20 splitting the dataset (80%->Training and 20%->Validation)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------LEXICAL FEATURES EXTRACTING----------\n",
      "\n",
      "successfully extracted all the features\n",
      " \n",
      " \n",
      "--------LEXICAL FEATURES EXTRACTED AND STORED IN VECTOR------\n"
     ]
    }
   ],
   "source": [
    "print(\"------------------LEXICAL FEATURES EXTRACTING----------\")\n",
    "print()\n",
    "author_fvs={}\n",
    "feature_vectors=[]\n",
    "for (text,author) in zip(X_train, y_train):\n",
    "    author_fvs[author]=FeatureExtration(text)\n",
    "    #feature_vectors.append(FeatureExtration(text))\n",
    "print(\"successfully extracted all the features\\n \\n \")\n",
    "print(\"--------LEXICAL FEATURES EXTRACTED AND STORED IN VECTOR------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.DataFrame()\n",
    "train_data[\"X_train\"] =X_train\n",
    "train_data[\"y_train\"] = y_train\n",
    "train_data[\"features\"] =author_fvs.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = pd.Series(train_data[\"y_train\"])\n",
    "Y.replace(r'(^ra.*)','ravibeligeri',regex=True, inplace = True)\n",
    "Y.replace(r'(^hr.*)','hrudayashiva',regex=True, inplace = True)\n",
    "Y.replace(r'(^som.*)','somashaker',regex=True, inplace = True)\n",
    "train_data[\"y_train\"] = Y\n",
    "y_train = train_data[\"y_train\"]\n",
    "vectors = train_data[\"features\"]\n",
    "# labelencoder = LabelEncoder()\n",
    "# y = labelencoder.fit_transform(y)\n",
    "arr = [0,0,0,0,0,0]\n",
    "for i in vectors:\n",
    "    for k in range(500-len(i)):\n",
    "        i.append(arr)\n",
    "arr = (np.array(vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# mean normalization of the data . converting into normal distribution having mean=0 , -0.1<x<0.1\n",
    "#sc = StandardScaler().fit(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed Flat List 28\n"
     ]
    }
   ],
   "source": [
    "def flatten_list(_2d_list):\n",
    "    flat_list = []\n",
    "    # Iterate through the outer list\n",
    "    for element in _2d_list:\n",
    "        if type(element) is list:\n",
    "            # If the element is of type list, iterate through the sublist\n",
    "            for item in element:\n",
    "                flat_list.append(item)\n",
    "        else:\n",
    "            flat_list.append(element)\n",
    "    return flat_list\n",
    "flatten_final=[]\n",
    "for i in range(len(vectors)):\n",
    "    flatten_final.append(flatten_list(arr[i]))\n",
    "\n",
    "# nested_list = vectors[0]\n",
    "# print('Original List', len(nested_list)\n",
    "print('Transformed Flat List', len(flatten_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "flatten_final = np.asarray(flatten_final)\n",
    "a = flatten_final\n",
    "col_mean = np.nanmean(a, axis=0)\n",
    "#Find indices that you need to replace\n",
    "inds = np.where(np.isnan(a))\n",
    "#Place column means in the indices. Align the arrays using take\n",
    "a[inds] = np.take(col_mean, inds[1])\n",
    "flatten_final = a\n",
    "flatten_final=sparse.csr_matrix(flatten_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# steps = [(flatten_final)]\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# pipeline = Pipeline(steps) # define the pipeline object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_t=[]\n",
    "for i in X_test:\n",
    "    X_test_t.append(FeatureExtration(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr1 = [0,0,0,0,0,0]\n",
    "for i in X_test_t:\n",
    "    for k in range(500-len(i)):\n",
    "        i.append(pd.Series(arr1))\n",
    "arr2 = (np.array(X_test_t))\n",
    "test_flaten=[]\n",
    "for i in range(len(arr2)):\n",
    "    test_flaten.append(flatten_list(arr2[i]))\n",
    "    \n",
    "test_flaten = np.asarray(test_flaten)\n",
    "\n",
    "c = test_flaten\n",
    "col_mean = np.nanmean(c, axis=0)\n",
    "#Find indices that you need to replace\n",
    "inds = np.where(np.isnan(c))\n",
    "#Place column means in the indices. Align the arrays using take\n",
    "c[inds] = np.take(col_mean, inds[1])\n",
    "test_flaten = c\n",
    "test_flaten = test_flaten.reshape(8, 3000)\n",
    "test_flaten=sparse.csr_matrix(test_flaten)\n",
    "z= pd.Series(y_test)\n",
    "z.replace(r'(^ra.*)','ravibeligeri',regex=True, inplace = True)\n",
    "z.replace(r'(^hr.*)','hrudayashiva',regex=True, inplace = True)\n",
    "z.replace(r'(^som.*)','somashaker',regex=True, inplace = True)\n",
    "y_test =z\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Model buidling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taining Accuracy : 1.0\n",
      "Testing Accuracy:  0.5\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "#model =SVC(kernel='linear')\n",
    "#model = model.fit(flatten_final.toarray(), y_train)\n",
    "\n",
    "#model = GaussianNB()\n",
    "#model= model.fit(flatten_final.toarray(), y_train)\n",
    "\n",
    "#model = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "#model = model.fit(flatten_final.toarray(), y_train)\n",
    "\n",
    "model = DecisionTreeClassifier(random_state=0)\n",
    "model = model.fit(flatten_final.toarray(), y_train)\n",
    "print(\"Taining Accuracy :\",model.score(flatten_final.toarray(), y_train))\n",
    "print(\"Testing Accuracy: \",model.score(test_flaten.toarray(), y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "hrudayashiva       0.50      0.33      0.40         3\n",
      "ravibeligeri       0.50      0.50      0.50         2\n",
      "  somashaker       0.50      0.67      0.57         3\n",
      "\n",
      " avg / total       0.50      0.50      0.49         8\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "predictions = model.predict(test_flaten.toarray())\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------------------------------\n",
    "print(\"N-grams features- extracting \")\n",
    "word_vector=TfidfVectorizer(analyzer=\"word\", ngram_range=(2,3),max_features=3000, binary=False)\n",
    "char_vector=TfidfVectorizer(analyzer=\"char\", ngram_range=(2,3),max_features=3000, binary=False,min_df=0)\n",
    "vectorizer=FeatureUnion([ (\"chars\",char_vector),(\"word\",word_vector)] )\n",
    "trainset=[]\n",
    "classes=[]\n",
    "n_gram_fvs_all={}\n",
    "feature_names={}\n",
    "for author,file in instance_by_author.items():\n",
    "    trainset.append(file)\n",
    "    matrix = vectorizer.fit_transform(trainset)\n",
    "    n_gram_fvs_all[author]=matrix.toarray()\n",
    "    feature_names[author]=vectorizer.get_feature_names()\n",
    "    trainset.clear()\n",
    "    classes.append(author)\n",
    "    #print(\"matrix for the first document\\n\",matrix)\n",
    "# for lable, vector in n_gram_fvs_all.items():\n",
    "#     print(fileint(\"---------------n-grams features Extracted successfully-------------\")\n",
    "#------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "# Compare the disputed papers to those written by everyone,\n",
    "# including the shared ones.\n",
    "authors = instance_by_author.keys()\n",
    "\n",
    "# Transform the authors' corpora into lists of word tokens\n",
    "federalist_by_author_tokens = {}\n",
    "federalist_by_author_length_distributions = {}\n",
    "for author in authors:\n",
    "    tokens = nltk.word_tokenize(instance_by_author[author])\n",
    "    # Filter out punctuation\n",
    "    federalist_by_author_tokens[author] = ([token for token in tokens if any(c.isalpha() for c in token)])\n",
    "    #print(federalist_by_author_tokens[author])\n",
    "    # Get a distribution of token lengths\n",
    "    token_lengths = [len(token) for token in federalist_by_author_tokens[author]]\n",
    "    federalist_by_author_length_distributions[author] = nltk.FreqDist(token_lengths)\n",
    "    federalist_by_author_length_distributions[author].plot(15,title=author)\n",
    "    print(federalist_by_author_length_distributions[author], end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
