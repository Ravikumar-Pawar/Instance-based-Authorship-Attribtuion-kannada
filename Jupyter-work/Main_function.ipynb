{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Authorship identification is the task of identifying the author of a given text from a set of suspects. The main concern of this task is to define an appropriate characterization of texts that captures the writing style of authors.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* An approach, a set of documents written by a set of authors, create a system which, given a new unseen document, is able to tell the original author (from the available set of authors) of that document.\n",
    "\n",
    "* At present, there are a lot of Authorship Attribution Systems available online. But on a very high level, we can divide these systems into two categories.\n",
    "    1. Machine Learning Based\n",
    "    2. Deep Learning Based\n",
    "    \n",
    "* Machine learning based authorship attribution systems extract features like average length of words, the frequency of digits used, the frequency of letters used e.t.c. and use them to classify documents. On the other hand, deep learning based methods extract features on there own to classify documents.\n",
    "\n",
    "# Mainly two approaches are there in authorship attribution or authorship identification\n",
    "# 1. Profile-based Authorship Identification\n",
    "    All available text samples by one candidate author are treated cumulatively, that is they are concatenated in one big document and then a single representation is extracted to become the profile of the author. concatinating all documents of a author and treating as single document\n",
    "\n",
    "# 2. Instance-based Authorship Identifcation\n",
    "    All available samples by one author are treated separately. Each text sample has its own representation. Since these approaches are usually combined with discriminative machine learning algorithms, like support vector machines, they require multiple instances per class. Hence, when only one docu-ment is available for a candidate author, this document has to be split into multiple samples.\n",
    "\n",
    "* Here we are dealing with Instance based authorsh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instance-Based Authorship Attribution\n",
    "* in Instance-based AA, we have focused on Lexical Features and ngram features\n",
    "* both are written on separate moduel and imported here as python function <br>\n",
    "* **%run stylometric_Features.ipynb import FeatureExtration** it will extract the lexical features \n",
    "* **%run N_gram_Final.ipynb import ngramFeatureExtraction** it wil extract the ngram features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <div style=\"text-align: center; color:blue\"> importing necessary libraries</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* pandas, numpy, nltk, os,glob, sklearn etc.. these are the main important libraries to perform the operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using matplotlib backend: Qt5Agg\n"
     ]
    }
   ],
   "source": [
    "import io, os, sys, types\n",
    "import IPython\n",
    "%run stylometric_Features.ipynb import FeatureExtration\n",
    "%run N_gram_Final.ipynb import ngramFeatureExtraction\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import glob\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy import sparse\n",
    "import math\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import preprocessing\n",
    "import operator\n",
    "import itertools\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from stylometric_Features import LexicalFeatures,SyntacticFeatures\n",
    "#from stylometric_Features import FeatureExtration\n",
    "#from ngrams_Features import train_model\n",
    "\n",
    "#data reading mutilple files\n",
    "#hrudayashiva-instance-->01-12\n",
    "#ravi beligeri-isntance-->13-25\n",
    "#somashaker-isntance-->26-36"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <div style=\"text-align: center\"> reading file and extracting Lexical Features</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instance-based authorship attribution focuses on the individual documents in feature extracting! so here we have totlal 36 files with unique file names!\n",
    "these files are written  by Hrudayashiva, Ravibeligere and somashanakar; to distinguish each file, we have created a dictionary in python which can have seperate itarative itmes for each authors;\n",
    "suppose hrudayashiva we have 12 files, and numbered from 01 to 12\n",
    "* In Python, the glob module is used to retrieve files/pathnames matching a specified pattern.\n",
    "* The pattern rules of glob follow standard Unix path expansion rules.<br>\n",
    "* this is built in module in python no exeternal library required for this\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "all_instances={\n",
    "'hrudayashiva':[\"01\",\"02\",\"03\",\"04\",\"05\",\"06\",\"07\",\"08\",\"09\",10,11,12],\n",
    "    'ravibeliger':[13,14,15,16,17,18,19,20,21,22,23,24,25],\n",
    "    'somashaker':[26,27,28,29,30,31,32,33,34,35,36]\n",
    "}\n",
    "\n",
    "stop_words_list = \"C://Users/RAVIKUMAR/PycharmProjects/Authorship_Attribution_Instance/Data_2/new_stop_words.txt\"\n",
    "\n",
    "data_folder = \"C://Users/RAVIKUMAR/PycharmProjects/Authorship_Attribution_Instance/Data_2/train\"\n",
    "instance_by_author={}\n",
    "for author,AllFiles_Per_author in all_instances.items():\n",
    "    for i in AllFiles_Per_author:\n",
    "        for name in glob.glob(f\"C://Users/RAVIKUMAR/PycharmProjects/Authorship_Attribution_Instance/Data_2/train/instance{i}.txt\"):\n",
    "            with open(name, encoding='utf-8') as file:\n",
    "                instance_by_author[author+str(i)]=file.read()\n",
    "all_authors=[]\n",
    "text=[]\n",
    "original = pd.DataFrame()\n",
    "for author, file in instance_by_author.items():\n",
    "    all_authors.append(author)\n",
    "    text.append(file)\n",
    "    \n",
    "original[\"Authors with id\"] = all_authors\n",
    "original[\"text files\"]=text\n",
    "df=pd.DataFrame()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## created data frame  Authors with id\t and text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Authors with id</th>\n",
       "      <th>text files</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hrudayashiva01</td>\n",
       "      <td>\\nಹೋಳಿ ಹಬ್ಬದ ಸಂಭ್ರಮ, ಖುಷಿಗಳನ್ನು ನೀವೂ ಅನುಭವಿಸಿರ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hrudayashiva02</td>\n",
       "      <td>ಪ್ರಖ್ಯಾತ ಹಿಂದೂಸ್ತಾನಿ ಸಂಗೀತಗಾರ ಪಂಡಿತ್ ಪರಮೇಶ್ವರ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hrudayashiva03</td>\n",
       "      <td>\\nಟಾಮ್ ಹಾಂಕ್ಸ್ ಕ್ಯಾಲಿಫೋರ್ನಿಯಾದ ಪ್ರತಿಭಾವಂತ. ಈತ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hrudayashiva04</td>\n",
       "      <td>\\nಹೆಂಡತಿಯು ಹದ ಮಾಡಿಕೊಟ್ಟ ತಾಂಬೂಲ ಜಗಿಯುತ್ತಲೇ ಅವರು...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hrudayashiva05</td>\n",
       "      <td>\\n'ಈ ಹಾಡನ್ನು ಬೇರೆ ಯಾರಿಂದಲಾದರೂ ಬರೆಸಿಬಿಡಿ ಪ್ಲೀಸ್...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>hrudayashiva06</td>\n",
       "      <td>\\nನಾನು ಹದಿನಾರನೇ ವಯಸ್ಸಿಗೆ ಹಳ್ಳಿಗಳನ್ನು ಬಿಟ್ಟಿದ್ದ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>hrudayashiva07</td>\n",
       "      <td>\\nದೆವ್ವಗಳ ಬಗ್ಗೆ ಚರ್ಚಿಸುವುದೇ ವೇಳೆ ಹಾಳುಮಾಡಿಕೊಳ್ಳ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>hrudayashiva08</td>\n",
       "      <td>\\nಅನಂತಮೂರ್ತಿಯವರ ಕುರಿತು ಒಂದಿಷ್ಟು ಧ್ಯಾನಿಸುವ ಸಮಯವ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>hrudayashiva09</td>\n",
       "      <td>\\nಅದೊಂದು ಮಾಮೂಲಿ ದಿನ… ಆ ಸಂಜೆ ಒಂದು ಕಾರ್ಯಕ್ರಮಕ್ಕೆ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>hrudayashiva10</td>\n",
       "      <td>\\nದಟ್ಟ ಮೌನ ಕವಿದಿದೆ. ತಾತನ ಗೋರಿಯ ಪಕ್ಕದಲ್ಲಿ ಕುಳಿತ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>hrudayashiva11</td>\n",
       "      <td>\\nಕಗ್ಗಲಿಪುರದಲ್ಲಿ 'ಕರುನಾಡ ಗಜಕೇಸರಿ ಸೇನೆ'ಯ ಉದ್ಘಾಟ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>hrudayashiva12</td>\n",
       "      <td>\\n\\nಅದೇಕೋ ನೆನಪಾಗುತ್ತಿದೆ. ಐದಾರು ವರ್ಷಗಳ ಹಿಂದೆ ನಾ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ravibeliger13</td>\n",
       "      <td>\\nಆಕೆ ಟಿಕೆಟ್ ಕೌಂಟರ್‌ನ ಮುಂದೆ ನಿಂತು ಸಿನೆಮಾ ಚೆನ್ನ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ravibeliger14</td>\n",
       "      <td>ಇತ್ತೀಚೆಗೊಂದು ಮಧ್ಯಾಹ್ನ ದಾವಣಗೆರೆಯಿಂದ ರವೀಂದ್ರನಾಥ್...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ravibeliger15</td>\n",
       "      <td>\\n“ಅಬ್ಬ! ಎಂ.ಎ.? ಎಷ್ಟು ಕೊಡ್ತಾರಲೆ?\" ಕಣ್ಣರಳಿಸಿ ಕೇ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ravibeliger16</td>\n",
       "      <td>\\nಇವತ್ತು ಮಂಜು, ಸುಮಿತ್ರ, ಶಶಿಧರ ಎಲ್ಲರೂ ಬೆಂಗಳೂರಿನ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>ravibeliger17</td>\n",
       "      <td>\\nಗೆಳೆಯರು ಅವನನ್ನೇ ಆಶ್ಚರ್ಯದಿಂದ ನೋಡಿದರು. ಟೇಬಲ್‌ನ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ravibeliger18</td>\n",
       "      <td>\\nಈ ಹುಡುಗ ಸುದೀಪ್.\\n\\nಆತನ ಬಗ್ಗೆ ನನಗೊಂದು ಪ್ರೀತಿಯ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ravibeliger19</td>\n",
       "      <td>\\nಅವರೊಂದಿಗಿನ ನನ್ನ ಸ್ನೇಹ ಸುಮಾರು ಇಪ್ಪತ್ತು ವರ್ಷಗಳ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>ravibeliger20</td>\n",
       "      <td>\\nಕ್ಲಾಸಿನಲ್ಲಿರೋದೇ ಮೂರೂ ಮುಕ್ಕಾಲು ಜನ. ಅವರ ಪೈಕಿಯೇ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>ravibeliger21</td>\n",
       "      <td>ಜಾತಿಗಾ? ಮಾತಿಗಾ? ಅಷ್ಟು ದೊಡ್ಡ ಸಾಧಕರು ಅವರು. ಇನ್ನೇ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>ravibeliger22</td>\n",
       "      <td>ಅದಕ್ಕೆ ಯಾಕೆ ನಾನು ಹಾಗೆ ಹೆಸರಿಟ್ಟೆನೋ ಗೊತ್ತಿಲ್ಲ. ಆ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>ravibeliger23</td>\n",
       "      <td>\\nಹಾಗಂತ ಆಣೆ ಮಾಡಿ ಮನೆಯಿಂದ ಹೊರಬಿದ್ದಿರಬೇಕು ವಿಶ್ವೇ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>ravibeliger24</td>\n",
       "      <td>\\nನಾನೊಂದು ಹೊಸ ಕಾರು ತೆಗೊಂಡೆ, ನಮ್ಮನೆ ಬೆಕ್ಕು ಮರಿ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>ravibeliger25</td>\n",
       "      <td>\\nದೇಶದ ಹಲವು ರಾಜ್ಯಗಳಲ್ಲಿ ಆತ್ಮಹತ್ಯೆ ಮಾಡಿಕೊಳ್ಳುತ್...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>somashaker26</td>\n",
       "      <td>\\nಮನಸ್ಸು ಅದ್ಭುತವಾದುದು! ಅತ್ಯದ್ಬುತವಾದುದು! ಯಾರೂ ಅ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>somashaker27</td>\n",
       "      <td>\\nಬಹಳ ವರುಷ ಒಂದೇ ಸ್ಥಳದಲ್ಲಿ, ಒಂದೇ ವಾತಾವರಣದಲ್ಲಿ ವ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>somashaker28</td>\n",
       "      <td>\\nಮಾಂಗಲ್ಯಂ ತಂತುನಾನೇನ ಮಮಜೀವನ ಹೇತುನಾ … ಎಂಬ ಮಂತ್ರ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>somashaker29</td>\n",
       "      <td>\\nಜೀವನದಲ್ಲಿ ಮಾನವ ಅನೇಕ ಮುಖ್ಯ ಘಟ್ಟಗಳನ್ನು ದಾಟುತ್ತ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>somashaker30</td>\n",
       "      <td>\\nದಿನ ಪತ್ರಿಕೆಯ ಪುಟಗಳನ್ನು ತಿರುವಿ ಹಾಕಿದರೆ ಒಂದೆರಡ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>somashaker31</td>\n",
       "      <td>\\nಕನಸುಗಳು ಮನುಷ್ಯನನ್ನು ಪ್ರಾಚೀನ ಕಾಲದಿಂದಲೂ ವಿಸ್ಮಯ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>somashaker32</td>\n",
       "      <td>\\nಪ್ರಪಂಚ ರಹಸ್ಯಗಳ, ವಿಸ್ಮಯಗಳ ಆಗರ! ಎಲ್ಲಾ ಜೀವಿಗಳಂತ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>somashaker33</td>\n",
       "      <td>\\nಮಾನವನ ಬದುಕು ಸಾರ್ಥಕ ಆಗಬೇಕಾದರೆ ಅವನು ಏನನ್ನಾದರೂ ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>somashaker34</td>\n",
       "      <td>\\nಬದಲಾವಣೆ ಜಗದ ನಿಯಮ! ಪ್ರಕೃತಿ ನಿತ್ಯನೂತನ. ನವನವೀನ!...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>somashaker35</td>\n",
       "      <td>\\nಅರಿಷಡ್ವರ್ಗಗಳು ಮಾನವನನ್ನು ಉತ್ತಮನಾಗಲಿಕ್ಕೆ ಬಿಡುವ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>somashaker36</td>\n",
       "      <td>\\n \" ಇರುವುದೆಲ್ಲವ ಬಿಟ್ಟು ಇರದುದರೆಡೆಗೆ ತುಡಿವುದೇ ಜ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Authors with id                                         text files\n",
       "0   hrudayashiva01  \\nಹೋಳಿ ಹಬ್ಬದ ಸಂಭ್ರಮ, ಖುಷಿಗಳನ್ನು ನೀವೂ ಅನುಭವಿಸಿರ...\n",
       "1   hrudayashiva02  ಪ್ರಖ್ಯಾತ ಹಿಂದೂಸ್ತಾನಿ ಸಂಗೀತಗಾರ ಪಂಡಿತ್ ಪರಮೇಶ್ವರ ...\n",
       "2   hrudayashiva03  \\nಟಾಮ್ ಹಾಂಕ್ಸ್ ಕ್ಯಾಲಿಫೋರ್ನಿಯಾದ ಪ್ರತಿಭಾವಂತ. ಈತ ...\n",
       "3   hrudayashiva04  \\nಹೆಂಡತಿಯು ಹದ ಮಾಡಿಕೊಟ್ಟ ತಾಂಬೂಲ ಜಗಿಯುತ್ತಲೇ ಅವರು...\n",
       "4   hrudayashiva05  \\n'ಈ ಹಾಡನ್ನು ಬೇರೆ ಯಾರಿಂದಲಾದರೂ ಬರೆಸಿಬಿಡಿ ಪ್ಲೀಸ್...\n",
       "5   hrudayashiva06  \\nನಾನು ಹದಿನಾರನೇ ವಯಸ್ಸಿಗೆ ಹಳ್ಳಿಗಳನ್ನು ಬಿಟ್ಟಿದ್ದ...\n",
       "6   hrudayashiva07  \\nದೆವ್ವಗಳ ಬಗ್ಗೆ ಚರ್ಚಿಸುವುದೇ ವೇಳೆ ಹಾಳುಮಾಡಿಕೊಳ್ಳ...\n",
       "7   hrudayashiva08  \\nಅನಂತಮೂರ್ತಿಯವರ ಕುರಿತು ಒಂದಿಷ್ಟು ಧ್ಯಾನಿಸುವ ಸಮಯವ...\n",
       "8   hrudayashiva09  \\nಅದೊಂದು ಮಾಮೂಲಿ ದಿನ… ಆ ಸಂಜೆ ಒಂದು ಕಾರ್ಯಕ್ರಮಕ್ಕೆ...\n",
       "9   hrudayashiva10  \\nದಟ್ಟ ಮೌನ ಕವಿದಿದೆ. ತಾತನ ಗೋರಿಯ ಪಕ್ಕದಲ್ಲಿ ಕುಳಿತ...\n",
       "10  hrudayashiva11  \\nಕಗ್ಗಲಿಪುರದಲ್ಲಿ 'ಕರುನಾಡ ಗಜಕೇಸರಿ ಸೇನೆ'ಯ ಉದ್ಘಾಟ...\n",
       "11  hrudayashiva12  \\n\\nಅದೇಕೋ ನೆನಪಾಗುತ್ತಿದೆ. ಐದಾರು ವರ್ಷಗಳ ಹಿಂದೆ ನಾ...\n",
       "12   ravibeliger13  \\nಆಕೆ ಟಿಕೆಟ್ ಕೌಂಟರ್‌ನ ಮುಂದೆ ನಿಂತು ಸಿನೆಮಾ ಚೆನ್ನ...\n",
       "13   ravibeliger14  ಇತ್ತೀಚೆಗೊಂದು ಮಧ್ಯಾಹ್ನ ದಾವಣಗೆರೆಯಿಂದ ರವೀಂದ್ರನಾಥ್...\n",
       "14   ravibeliger15  \\n“ಅಬ್ಬ! ಎಂ.ಎ.? ಎಷ್ಟು ಕೊಡ್ತಾರಲೆ?\" ಕಣ್ಣರಳಿಸಿ ಕೇ...\n",
       "15   ravibeliger16  \\nಇವತ್ತು ಮಂಜು, ಸುಮಿತ್ರ, ಶಶಿಧರ ಎಲ್ಲರೂ ಬೆಂಗಳೂರಿನ...\n",
       "16   ravibeliger17  \\nಗೆಳೆಯರು ಅವನನ್ನೇ ಆಶ್ಚರ್ಯದಿಂದ ನೋಡಿದರು. ಟೇಬಲ್‌ನ...\n",
       "17   ravibeliger18  \\nಈ ಹುಡುಗ ಸುದೀಪ್.\\n\\nಆತನ ಬಗ್ಗೆ ನನಗೊಂದು ಪ್ರೀತಿಯ...\n",
       "18   ravibeliger19  \\nಅವರೊಂದಿಗಿನ ನನ್ನ ಸ್ನೇಹ ಸುಮಾರು ಇಪ್ಪತ್ತು ವರ್ಷಗಳ...\n",
       "19   ravibeliger20  \\nಕ್ಲಾಸಿನಲ್ಲಿರೋದೇ ಮೂರೂ ಮುಕ್ಕಾಲು ಜನ. ಅವರ ಪೈಕಿಯೇ...\n",
       "20   ravibeliger21  ಜಾತಿಗಾ? ಮಾತಿಗಾ? ಅಷ್ಟು ದೊಡ್ಡ ಸಾಧಕರು ಅವರು. ಇನ್ನೇ...\n",
       "21   ravibeliger22  ಅದಕ್ಕೆ ಯಾಕೆ ನಾನು ಹಾಗೆ ಹೆಸರಿಟ್ಟೆನೋ ಗೊತ್ತಿಲ್ಲ. ಆ...\n",
       "22   ravibeliger23  \\nಹಾಗಂತ ಆಣೆ ಮಾಡಿ ಮನೆಯಿಂದ ಹೊರಬಿದ್ದಿರಬೇಕು ವಿಶ್ವೇ...\n",
       "23   ravibeliger24  \\nನಾನೊಂದು ಹೊಸ ಕಾರು ತೆಗೊಂಡೆ, ನಮ್ಮನೆ ಬೆಕ್ಕು ಮರಿ ...\n",
       "24   ravibeliger25  \\nದೇಶದ ಹಲವು ರಾಜ್ಯಗಳಲ್ಲಿ ಆತ್ಮಹತ್ಯೆ ಮಾಡಿಕೊಳ್ಳುತ್...\n",
       "25    somashaker26  \\nಮನಸ್ಸು ಅದ್ಭುತವಾದುದು! ಅತ್ಯದ್ಬುತವಾದುದು! ಯಾರೂ ಅ...\n",
       "26    somashaker27  \\nಬಹಳ ವರುಷ ಒಂದೇ ಸ್ಥಳದಲ್ಲಿ, ಒಂದೇ ವಾತಾವರಣದಲ್ಲಿ ವ...\n",
       "27    somashaker28  \\nಮಾಂಗಲ್ಯಂ ತಂತುನಾನೇನ ಮಮಜೀವನ ಹೇತುನಾ … ಎಂಬ ಮಂತ್ರ...\n",
       "28    somashaker29  \\nಜೀವನದಲ್ಲಿ ಮಾನವ ಅನೇಕ ಮುಖ್ಯ ಘಟ್ಟಗಳನ್ನು ದಾಟುತ್ತ...\n",
       "29    somashaker30  \\nದಿನ ಪತ್ರಿಕೆಯ ಪುಟಗಳನ್ನು ತಿರುವಿ ಹಾಕಿದರೆ ಒಂದೆರಡ...\n",
       "30    somashaker31  \\nಕನಸುಗಳು ಮನುಷ್ಯನನ್ನು ಪ್ರಾಚೀನ ಕಾಲದಿಂದಲೂ ವಿಸ್ಮಯ...\n",
       "31    somashaker32  \\nಪ್ರಪಂಚ ರಹಸ್ಯಗಳ, ವಿಸ್ಮಯಗಳ ಆಗರ! ಎಲ್ಲಾ ಜೀವಿಗಳಂತ...\n",
       "32    somashaker33  \\nಮಾನವನ ಬದುಕು ಸಾರ್ಥಕ ಆಗಬೇಕಾದರೆ ಅವನು ಏನನ್ನಾದರೂ ...\n",
       "33    somashaker34  \\nಬದಲಾವಣೆ ಜಗದ ನಿಯಮ! ಪ್ರಕೃತಿ ನಿತ್ಯನೂತನ. ನವನವೀನ!...\n",
       "34    somashaker35  \\nಅರಿಷಡ್ವರ್ಗಗಳು ಮಾನವನನ್ನು ಉತ್ತಮನಾಗಲಿಕ್ಕೆ ಬಿಡುವ...\n",
       "35    somashaker36  \\n \" ಇರುವುದೆಲ್ಲವ ಬಿಟ್ಟು ಇರದುದರೆಡೆಗೆ ತುಡಿವುದೇ ಜ..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split the data into training and testing <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labelencoder = LabelEncoder()\n",
    "# y = labelencoder.fit_transform(y)\n",
    "X=original[\"text files\"]\n",
    "y=all_authors\n",
    "# 80-20 splitting the dataset (80%->Training and 20%->Validation)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* from sklearn.model_selection import **train_test_split** <br>**train_test_split()** is built in method in sklearn, which will split the given data into training and testing with custom size;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------LEXICAL FEATURES EXTRACTING----------\n",
      "\n",
      "successfully extracted all the features\n",
      " \n",
      " \n"
     ]
    }
   ],
   "source": [
    "print(\"------------------LEXICAL FEATURES EXTRACTING----------\")\n",
    "print()\n",
    "author_fvs={}\n",
    "feature_vectors=[]\n",
    "for (text,author) in zip(X_train, y_train):\n",
    "    author_fvs[author]=FeatureExtration(text)\n",
    "    #feature_vectors.append(FeatureExtration(text))\n",
    "print(\"successfully extracted all the features\\n \\n \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extracting <br>this is Major step in authorship attribution ----Feature Extraction\n",
    "text Data converting into Numerical Data<br>\n",
    "In this authorship approach we have extracted **Lexical features** and **n-gram features**\n",
    "* **FeatureExtration()** is method created in another jupyter notebook file\n",
    "which will take a input text file and process it, and then extract the feautures and stored in a vector\n",
    "This process involves iterating all the training files to extract the lexical features and stored in a vector<br>\n",
    "this vector is mapped to unique author id<br>\n",
    "* Ex: hrudayashiva03: [list of all features of this particular file],  like that all the training files processed and all features extracted\n",
    "\n",
    "# lexical features extracted here\n",
    "* FeatureExtration(instances) this function will extract all the lexical features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.DataFrame()\n",
    "train_data[\"X_train\"] =X_train\n",
    "train_data[\"y_train\"] = y_train\n",
    "train_data[\"features\"] =author_fvs.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = pd.Series(train_data[\"y_train\"])\n",
    "Y.replace(r'(^ra.*)','ravibeligeri',regex=True, inplace = True)\n",
    "Y.replace(r'(^hr.*)','hrudayashiva',regex=True, inplace = True)\n",
    "Y.replace(r'(^som.*)','somashaker',regex=True, inplace = True)\n",
    "train_data[\"y_train\"] = Y\n",
    "\n",
    "y_train = train_data[\"y_train\"]\n",
    "vectors = train_data[\"features\"]\n",
    "# labelencoder = LabelEncoder()\n",
    "# y = labelencoder.fit_transform(y)\n",
    "arr = [0,0,0,0,0,0]\n",
    "for i in vectors:\n",
    "    for k in range(500-len(i)):\n",
    "        i.append(arr)\n",
    "arr = (np.array(vectors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X_train</th>\n",
       "      <th>y_train</th>\n",
       "      <th>features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>ಜಾತಿಗಾ? ಮಾತಿಗಾ? ಅಷ್ಟು ದೊಡ್ಡ ಸಾಧಕರು ಅವರು. ಇನ್ನೇ...</td>\n",
       "      <td>ravibeligeri</td>\n",
       "      <td>[[6.0, 7.0, 1.0, 0.0, 0.14285714285714285, 1.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\nಹೋಳಿ ಹಬ್ಬದ ಸಂಭ್ರಮ, ಖುಷಿಗಳನ್ನು ನೀವೂ ಅನುಭವಿಸಿರ...</td>\n",
       "      <td>hrudayashiva</td>\n",
       "      <td>[[7.5, 53.0, 6.0, 0.018867924528301886, 0.0377...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>\\nಅನಂತಮೂರ್ತಿಯವರ ಕುರಿತು ಒಂದಿಷ್ಟು ಧ್ಯಾನಿಸುವ ಸಮಯವ...</td>\n",
       "      <td>hrudayashiva</td>\n",
       "      <td>[[8.6, 49.0, 5.0, 0.02040816326530612, 0.02040...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\nಹೆಂಡತಿಯು ಹದ ಮಾಡಿಕೊಟ್ಟ ತಾಂಬೂಲ ಜಗಿಯುತ್ತಲೇ ಅವರು...</td>\n",
       "      <td>hrudayashiva</td>\n",
       "      <td>[[7.8, 133.0, 15.0, 0.007518796992481203, 0.00...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>\\nಕಗ್ಗಲಿಪುರದಲ್ಲಿ 'ಕರುನಾಡ ಗಜಕೇಸರಿ ಸೇನೆ'ಯ ಉದ್ಘಾಟ...</td>\n",
       "      <td>hrudayashiva</td>\n",
       "      <td>[[7.111111111111111, 76.0, 9.0, 0.039473684210...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              X_train       y_train  \\\n",
       "20  ಜಾತಿಗಾ? ಮಾತಿಗಾ? ಅಷ್ಟು ದೊಡ್ಡ ಸಾಧಕರು ಅವರು. ಇನ್ನೇ...  ravibeligeri   \n",
       "0   \\nಹೋಳಿ ಹಬ್ಬದ ಸಂಭ್ರಮ, ಖುಷಿಗಳನ್ನು ನೀವೂ ಅನುಭವಿಸಿರ...  hrudayashiva   \n",
       "7   \\nಅನಂತಮೂರ್ತಿಯವರ ಕುರಿತು ಒಂದಿಷ್ಟು ಧ್ಯಾನಿಸುವ ಸಮಯವ...  hrudayashiva   \n",
       "3   \\nಹೆಂಡತಿಯು ಹದ ಮಾಡಿಕೊಟ್ಟ ತಾಂಬೂಲ ಜಗಿಯುತ್ತಲೇ ಅವರು...  hrudayashiva   \n",
       "10  \\nಕಗ್ಗಲಿಪುರದಲ್ಲಿ 'ಕರುನಾಡ ಗಜಕೇಸರಿ ಸೇನೆ'ಯ ಉದ್ಘಾಟ...  hrudayashiva   \n",
       "\n",
       "                                             features  \n",
       "20  [[6.0, 7.0, 1.0, 0.0, 0.14285714285714285, 1.0...  \n",
       "0   [[7.5, 53.0, 6.0, 0.018867924528301886, 0.0377...  \n",
       "7   [[8.6, 49.0, 5.0, 0.02040816326530612, 0.02040...  \n",
       "3   [[7.8, 133.0, 15.0, 0.007518796992481203, 0.00...  \n",
       "10  [[7.111111111111111, 76.0, 9.0, 0.039473684210...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After extracting lexical features, we will be removeving the **authors id** to **Actual Author Name** for classification ease; <br>\n",
    "Ex: **hrudayashiva01--12=Hrudayashiva** <br>\n",
    "* all the extracted features are not in same size, for processing and shaping conviniet we have added additional zeros to make all the vectors are in same size;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# mean normalization of the data . converting into normal distribution having mean=0 , -0.1<x<0.1\n",
    "#sc = StandardScaler().fit(arr)\n",
    "#train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed Flat List 28\n"
     ]
    }
   ],
   "source": [
    "def flatten_list(_2d_list):\n",
    "    flat_list = []\n",
    "    # Iterate through the outer list\n",
    "    for element in _2d_list:\n",
    "        if type(element) is list:\n",
    "            # If the element is of type list, iterate through the sublist\n",
    "            for item in element:\n",
    "                flat_list.append(item)\n",
    "        else:\n",
    "            flat_list.append(element)\n",
    "    return flat_list\n",
    "flatten_final=[]\n",
    "for i in range(len(vectors)):\n",
    "    flatten_final.append(flatten_list(arr[i]))\n",
    "\n",
    "# nested_list = vectors[0]\n",
    "# print('Original List', len(nested_list)\n",
    "print('Transformed Flat List', len(flatten_final))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flattening a list of lists entails converting a 2D list into a 1D list by un-nesting each list item stored in the list of lists\n",
    "this ensures the concatinating all the features of same file; prior we have extracted features and each feature is in it's own list, it creates nested list;<br> now we will be converting 2D to 1D, i.e Gathering all the features belong to same file into a single vector for better convininent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "flatten_final = np.asarray(flatten_final)\n",
    "a = flatten_final\n",
    "col_mean = np.nanmean(a, axis=0)\n",
    "#Find indices that you need to replace\n",
    "inds = np.where(np.isnan(a))\n",
    "#Place column means in the indices. Align the arrays using take\n",
    "a[inds] = np.take(col_mean, inds[1])\n",
    "flatten_final = a\n",
    "flatten_final=sparse.csr_matrix(flatten_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* removing NaN values from the flatten array with mean of columns <br>\n",
    "after removing all the NaN values , created sparse matrix for the non zero values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* trained data get converted into sparse matrix: <br> **<28x3000 sparse matrix of type '<class 'numpy.float64'>'**\n",
    "\twith 13045 stored elements in Compressed Sparse Row format>\n",
    "* same entire process is applid to test data to **convert raw text data into numberical format**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_t=[]\n",
    "for i in X_test:\n",
    "    X_test_t.append(FeatureExtration(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr1 = [0,0,0,0,0,0]\n",
    "for i in X_test_t:\n",
    "    for k in range(500-len(i)):\n",
    "        i.append(pd.Series(arr1))\n",
    "arr1 = (np.array(X_test_t))\n",
    "test_flaten=[]\n",
    "for i in range(len(arr1)):\n",
    "    test_flaten.append(flatten_list(arr1[i]))\n",
    "    \n",
    "test_flaten = np.asarray(test_flaten)\n",
    "c = test_flaten\n",
    "col_mean = np.nanmean(c, axis=0)\n",
    "#Find indices that you need to replace\n",
    "inds = np.where(np.isnan(c))\n",
    "#Place column means in the indices. Align the arrays using take\n",
    "c[inds] = np.take(col_mean, inds[1])\n",
    "test_flaten = c\n",
    "\n",
    "test_flaten = test_flaten.reshape(8, 3000)\n",
    "test_flaten=sparse.csr_matrix(test_flaten)\n",
    "#-------------------------------------------------------------------------\n",
    "z= pd.Series(y_test)\n",
    "z.replace(r'(^ra.*)','ravibeligeri',regex=True, inplace = True)\n",
    "z.replace(r'(^hr.*)','hrudayashiva',regex=True, inplace = True)\n",
    "z.replace(r'(^som.*)','somashaker',regex=True, inplace = True)\n",
    "y_test =z\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# same procees applid to test data to convert numerical format "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# <div style=\"text-align:center\">Model Budilding and Result:Lexical Feature Model</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taining Accuracy : 1.0\n",
      "Testing Accuracy:  0.5\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# model =SVC(kernel='linear')\n",
    "# model = model.fit(flatten_final.toarray(), y_train)\n",
    "\n",
    "#model = GaussianNB()\n",
    "#model= model.fit(flatten_final.toarray(), y_train)\n",
    "\n",
    "# model = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "# model = model.fit(flatten_final.toarray(), y_train)\n",
    "\n",
    "model = DecisionTreeClassifier(random_state=0)\n",
    "model = model.fit(flatten_final.toarray(), y_train)\n",
    "print(\"Taining Accuracy :\",model.score(flatten_final.toarray(), y_train))\n",
    "print(\"Testing Accuracy: \",model.score(test_flaten.toarray(), y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div style=\"text-align:center\">Lexical features: classification report</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "hrudayashiva       0.50      0.33      0.40         3\n",
      "ravibeligeri       0.50      0.50      0.50         2\n",
      "  somashaker       0.50      0.67      0.57         3\n",
      "\n",
      " avg / total       0.50      0.50      0.49         8\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "predictions = model.predict(test_flaten.toarray())\n",
    "print(classification_report(y_test, predictions))\n",
    "\n",
    "\n",
    "lexicalPredication=pd.DataFrame()\n",
    "lexicalPredication['test files']=X_test\n",
    "lexicalPredication['Authors Predicted']=predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#original"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <div style=\"text-align:center\">n-gram Feature Extraction and model building</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N-grams features- extracting \n",
      "SVM model used here....\n",
      "Training Accuracy : 1.0\n",
      "Testing Acuuracy  :  0.875\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "hrudayashiva       1.00      0.67      0.80         3\n",
      "ravibeligeri       0.67      1.00      0.80         2\n",
      "  somashaker       1.00      1.00      1.00         3\n",
      "\n",
      " avg / total       0.92      0.88      0.88         8\n",
      "\n",
      "['hrudayashiva' 'somashaker' 'ravibeligeri' 'ravibeligeri' 'hrudayashiva'\n",
      " 'somashaker' 'ravibeligeri' 'somashaker'] \n",
      "\n",
      "---------------n-grams features Extracted successfully-------------\n"
     ]
    }
   ],
   "source": [
    "#---------------------------------------------------------------------------------------------\n",
    "print(\"N-grams features- extracting \")\n",
    "ngramFeatureExtraction()\n",
    "print(\"---------------n-grams features Extracted successfully-------------\")\n",
    "#------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# %matplotlib inline\n",
    "# import seaborn as sns\n",
    "# # Compare the disputed papers to those written by everyone,\n",
    "# # including the shared ones.\n",
    "# authors = instance_by_author.keys()\n",
    "\n",
    "# # Transform the authors' corpora into lists of word tokens\n",
    "# federalist_by_author_tokens = {}\n",
    "# federalist_by_author_length_distributions = {}\n",
    "# for author in authors:\n",
    "#     tokens = nltk.word_tokenize(instance_by_author[author])\n",
    "#     # Filter out punctuation\n",
    "#     federalist_by_author_tokens[author] = ([token for token in tokens if any(c.isalpha() for c in token)])\n",
    "#     #print(federalist_by_author_tokens[author])\n",
    "#     # Get a distribution of token lengths\n",
    "#     token_lengths = [len(token) for token in federalist_by_author_tokens[author]]\n",
    "#     federalist_by_author_length_distributions[author] = nltk.FreqDist(token_lengths)\n",
    "#     federalist_by_author_length_distributions[author].plot(15,title=author)\n",
    "#     print(federalist_by_author_length_distributions[author], end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
