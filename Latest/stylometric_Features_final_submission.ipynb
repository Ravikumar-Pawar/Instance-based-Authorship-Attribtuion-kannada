{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   #                    stylometric Features Extraction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <font color=\"blue\">importing required libraries</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections as coll\n",
    "import math\n",
    "import pickle\n",
    "import string\n",
    "import numpy as np\n",
    "from nltk.corpus import cmudict\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "cmuDictionary = None\n",
    "from os import  path\n",
    "\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import math\n",
    "\n",
    "\n",
    "import io, os, sys, types\n",
    "import IPython\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import glob\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy import sparse\n",
    "import math\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import preprocessing\n",
    "import operator\n",
    "import itertools\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_list=\"C:/Users/RAVIKUMAR/PycharmProjects/Authorship_Attribution_Instance/Data_2/new_stop_words.txt\"\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# removing stop words plus punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(stop_words_list, encoding='utf-8') as file:\n",
    "    reader=file.read()\n",
    "    reader = [reader.split()]\n",
    "    stopword = sum(reader, [])\n",
    "#ss =\".,\"    \n",
    "def text_process(text):\n",
    "    nopunct = \"\".join([char for char in text if char not in string.punctuation])\n",
    "    tokens = nltk.word_tokenize(nopunct)\n",
    "    nopunct = \" \".join([word for word in tokens if word not in stopword])\n",
    "    return nopunct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Avg_wordLength(str):\n",
    "    norm=[]\n",
    "    str.translate(string.punctuation)\n",
    "    tokens = word_tokenize(str)\n",
    "    st = [\",\", \".\", \"'\", \"!\", '\"', \"#\", \"$\", \"%\", \"&\", \"(\", \")\", \"*\", \"+\", \"-\", \".\", \"/\", \":\", \";\", \"<\", \"=\", '>', \"?\",\n",
    "          \"@\", \"[\", \"\\\\\", \"]\", \"^\", \"_\", '`', \"{\", \"|\", \"}\", '~', '\\t', '\\n']\n",
    "    words = [word for word in tokens if word not in stopword and word not in st]\n",
    " \n",
    "    return np.average([len(word) for word in words])\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# returns avg number of characters in a sentence\n",
    "def Avg_SentLenghtByCh(text):\n",
    "    tokens = sent_tokenize(text)\n",
    "    return np.average([len(token) for token in tokens])\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "# returns avg number of words in a sentence\n",
    "def Avg_SentLenghtByWord(text):\n",
    "    tokens = sent_tokenize(text)\n",
    "    return np.average([len(token.split()) for token in tokens])\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "# COUNTS SPECIAL CHARACTERS NORMALIZED OVER LENGTH OF CHUNK\n",
    "def CountSpecialCharacter(text):\n",
    "    st = [\"#\", \"$\", \"%\", \"&\", \"(\", \")\", \"*\", \"+\", \"-\", \"/\", \"<\", \"=\", '>',\n",
    "          \"@\", \"[\", \"\\\\\", \"]\", \"^\", \"_\", '`', \"{\", \"|\", \"}\", '~', '\\t', '\\n']\n",
    "    count = 0\n",
    "    for i in text:\n",
    "        if (i in st):\n",
    "            count = count + 1\n",
    "    return count / len(text)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "def CountPuncuation(text):\n",
    "    st = [\",\", \".\", \"'\", \"!\", '\"', \";\", \"?\", \":\", \";\"]\n",
    "    count = 0\n",
    "    for i in text:\n",
    "        if (i in st):\n",
    "            count = count + 1\n",
    "    return float(count) / float(len(text))\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# also returns Honore Measure R\n",
    "def hapaxLegemena(text):\n",
    "    words = RemoveSpecialCHs(text)\n",
    "    V1 = 0\n",
    "    # dictionary comprehension . har word kay against value 0 kardi\n",
    "    freqs = {key: 0 for key in words}\n",
    "    for word in words:\n",
    "        freqs[word] += 1\n",
    "    for word in freqs:\n",
    "        if freqs[word] == 1:\n",
    "            V1 += 1\n",
    "    N = len(words)\n",
    "    V = float(len(set(words)))\n",
    "    if(N>0):\n",
    "        R = 100 * math.log(N) / max(1, (1 - (V1 / V)))\n",
    "        h = V1 / N\n",
    "    else:\n",
    "        R=0.00\n",
    "    return R\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def hapaxDisLegemena(text):\n",
    "    words = RemoveSpecialCHs(text)\n",
    "    count = 0\n",
    "    # Collections as coll Counter takes an iterable collapse duplicate and counts as\n",
    "    # a dictionary how many equivelant items has been entered\n",
    "    freqs = coll.Counter()\n",
    "    freqs.update(words)\n",
    "    for word in freqs:\n",
    "        if freqs[word] == 2:\n",
    "            count += 1\n",
    "    try:\n",
    "        h = count / float(len(words))\n",
    "        S = count / float(len(set(words)))\n",
    "        t = S+h\n",
    "    except:\n",
    "        t=0.00\n",
    "    return t\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "# c(w)  = ceil (log2 (f(w*)/f(w))) f(w*) frequency of most commonly used words f(w) frequency of word w\n",
    "# measure of vocabulary richness and connected to zipfs law, f(w*) const rak kay zips law say rank nikal rahay hein\n",
    "def AvgWordFrequencyClass(text):\n",
    "    words = RemoveSpecialCHs(text)\n",
    "    # dictionary comprehension . har word kay against value 0 kardi\n",
    "    freqs = {key: 0 for key in words}\n",
    "    for word in words:\n",
    "        freqs[word] += 1\n",
    "    try:\n",
    "        maximum = float(max(list(freqs.values())))\n",
    "    except:\n",
    "        maximum = 0.00\n",
    "    return np.average([math.floor(math.log((maximum + 1) / (freqs[word]) + 1, 2)) for word in words])\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# TYPE TOKEN RATIO NO OF DIFFERENT WORDS / NO OF WORDS\n",
    "def typeTokenRatio(text):\n",
    "    words = word_tokenize(text)\n",
    "    return len(set(words)) / len(words)\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# logW = V-a/log(N)\n",
    "# N = total words , V = vocabulary richness (unique words) ,  a=0.17\n",
    "# we can convert into log because we are only comparing different texts\n",
    "def BrunetsMeasureW(text):\n",
    "    words = RemoveSpecialCHs(text)\n",
    "    a = 0.17\n",
    "    V = float(len(set(words)))\n",
    "    N = len(words)\n",
    "    try:\n",
    "        B = (V - a) / (math.log(N))\n",
    "    except:\n",
    "        B=0.00\n",
    "        \n",
    "    return B\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "def RemoveSpecialCHs(text):\n",
    "    text = word_tokenize(text)\n",
    "    st = [\",\", \".\", \"'\", \"!\", '\"', \"#\", \"$\", \"%\", \"&\", \"(\", \")\", \"*\", \"+\", \"-\", \".\", \"/\", \":\", \";\", \"<\", \"=\", '>', \"?\",\n",
    "          \"@\", \"[\", \"\\\\\", \"]\", \"^\", \"_\", '`', \"{\", \"|\", \"}\", '~', '\\t', '\\n']\n",
    "\n",
    "    words = [word for word in text if word not in st]\n",
    "    return words\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# K  10,000 * (M - N) / N**2\n",
    "# , where M  Sigma i**2 * Vi.\n",
    "def YulesCharacteristicK(text):\n",
    "    words = RemoveSpecialCHs(text)\n",
    "    N = len(words)\n",
    "    freqs = coll.Counter()\n",
    "    freqs.update(words)\n",
    "    vi = coll.Counter()\n",
    "    vi.update(freqs.values())\n",
    "    M = sum([(value * value) * vi[value] for key, value in freqs.items()])\n",
    "    try:\n",
    "        K = 10000 * (M - N) / math.pow(N, 2)\n",
    "    except:\n",
    "        K = 0.00\n",
    "    return K\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# -1*sigma(pi*lnpi)\n",
    "# Shannon and sympsons index are basically diversity indices for any community\n",
    "def ShannonEntropy(text):\n",
    "    words = RemoveSpecialCHs(text)\n",
    "    lenght = len(words)\n",
    "    freqs = coll.Counter()\n",
    "    freqs.update(words)\n",
    "    arr = np.array(list(freqs.values()))\n",
    "    distribution = 1. * arr\n",
    "    distribution /= max(1, lenght)\n",
    "    import scipy as sc\n",
    "    H = sc.stats.entropy(distribution, base=2)\n",
    "    # H = sum([(i/lenght)*math.log(i/lenght,math.e) for i in freqs.values()])\n",
    "    return H\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1 - (sigma(n(n - 1))/N(N-1)\n",
    "# N is total number of words\n",
    "# n is the number of each type of word\n",
    "def SimpsonsIndex(text):\n",
    "    words = RemoveSpecialCHs(text)\n",
    "    freqs = coll.Counter()\n",
    "    freqs.update(words)\n",
    "    N = len(words)\n",
    "    n = sum([1.0 * i * (i - 1) for i in freqs.values()])\n",
    "    try:\n",
    "        D = 1 - (n / (N * (N - 1)))\n",
    "    except:\n",
    "        D = 0.00\n",
    "    return D\n",
    "\n",
    "\n",
    "def Each_sentence(sequence):\n",
    "    sequence = sent_tokenize(sequence)\n",
    "    return sequence\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FeatureExtration(text):\n",
    "    # cmu dictionary for syllables\n",
    "    df = pd.DataFrame()\n",
    "    global cmuDictionary\n",
    "    cmuDictionary = cmudict.dict()\n",
    "    All_sentences_per_instance = Each_sentence(text)\n",
    "    vector = []\n",
    "    for line in All_sentences_per_instance:\n",
    "        feature = []\n",
    "        \n",
    "        meanwl = (Avg_wordLength(line))\n",
    "        feature.append(meanwl)\n",
    "        \n",
    "        meansl = (Avg_SentLenghtByCh(line))\n",
    "        feature.append(meansl)\n",
    "        \n",
    "        mean = (Avg_SentLenghtByWord(line))\n",
    "        feature.append(mean)\n",
    "        \n",
    "        means = CountSpecialCharacter(line)\n",
    "        feature.append(means)\n",
    "        \n",
    "        p = CountPuncuation(line)\n",
    "        feature.append(p)\n",
    "        \n",
    "        TTratio = typeTokenRatio(line)\n",
    "        feature.append(TTratio)\n",
    "        \n",
    "        #-----------new feature adding-----------\n",
    "        h = hapaxLegemena(line)\n",
    "        feature.append(h)\n",
    "        \n",
    "        B = BrunetsMeasureW(line)\n",
    "        feature.append(B)\n",
    "        \n",
    "        Y = YulesCharacteristicK(line)\n",
    "        feature.append(Y)\n",
    "        \n",
    "        S = ShannonEntropy(line)\n",
    "        feature.append(S)\n",
    "        \n",
    "        SI =SimpsonsIndex(line)\n",
    "        feature.append(SI)\n",
    "        \n",
    "        \n",
    "        AWF = AvgWordFrequencyClass(line)\n",
    "        feature.append(AWF)\n",
    "        \n",
    "        h = hapaxDisLegemena(line)\n",
    "        feature.append(h)\n",
    "        \n",
    "        vector.append(feature)\n",
    "    return vector\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "all_instances={\n",
    "'hrudayashiva':[\"01\",\"02\",\"03\",\"04\",\"05\",\"06\",\"07\",\"08\",\"09\",10,11,12],\n",
    "    'ravibeliger':[13,14,15,16,17,18,19,20,21,22,23,24,25],\n",
    "    'somashaker':[26,27,28,29,30,31,32,33,34,35,36],\n",
    "    'chandrashekark':[37,38,39,40,41,42,43,44,45,46,47],\n",
    "    'usha':[48,49,50,51,52,53,54,55,56,57,58,59,60],\n",
    "    'bhagavan':[61,62,63,64,65,66,67,68,69,70,71,72],\n",
    "    'NaDisoza':[73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92],\n",
    "    'srinatha':[93,94,95,96,97,98,99,100]\n",
    "}\n",
    "\n",
    "# stop_words_list = \"C://Users/RAVIKUMAR/PycharmProjects/Authorship_Attribution_Instance/Data_2/new_stop_words.txt\"\n",
    "\n",
    "\n",
    "instance_by_author={}\n",
    "for author,AllFiles_Per_author in all_instances.items():\n",
    "    for i in AllFiles_Per_author:\n",
    "        for name in glob.glob(f\"C://Users/RAVIKUMAR/PycharmProjects/Authorship_Attribution_Instance/Data_2/train/instance{i}.txt\"):\n",
    "            with open(name, encoding='utf-8') as file:\n",
    "                instance_by_author[author+str(i)]=file.read()\n",
    "all_authors=[]\n",
    "text=[]\n",
    "original = pd.DataFrame()\n",
    "for author, file in instance_by_author.items():\n",
    "    all_authors.append(author)\n",
    "    text.append(file)\n",
    "    \n",
    "original[\"Authors with id\"] = all_authors\n",
    "original[\"text files\"]=text\n",
    "df=pd.DataFrame()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=original[\"text files\"]\n",
    "y=all_authors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------LEXICAL FEATURES EXTRACTING----------\n",
      "\n",
      "successfully extracted all the features\n",
      " \n",
      " \n"
     ]
    }
   ],
   "source": [
    "print(\"------------------LEXICAL FEATURES EXTRACTING----------\")\n",
    "print()\n",
    "author_fvs={}\n",
    "feature_vectors=[]\n",
    "for (text,author) in zip(X, y):\n",
    "   # clean_text = text_process(text)\n",
    "    #print(author, clean_text)\n",
    "    author_fvs[author]=FeatureExtration(text)\n",
    "    #feature_vectors.append(FeatureExtration(text))\n",
    "    \n",
    "print(\"successfully extracted all the features\\n \\n \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# avgValues={}\n",
    "# def column_sum(lst):   \n",
    "#     return list(map(sum, zip(*lst)))\n",
    "    \n",
    "# for i, j in author_fvs.items():\n",
    "#     avgValues[i] = column_sum(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.DataFrame()\n",
    "train_data[\"X_train\"] =X\n",
    "train_data[\"y_train\"] = y\n",
    "train_data[\"features\"] =author_fvs.values()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = pd.Series(train_data[\"y_train\"])\n",
    "Y.replace(r'(^ra.*)','Ravibeligeri',regex=True, inplace = True)\n",
    "Y.replace(r'(^hr.*)','Hrudayashiva',regex=True, inplace = True)\n",
    "Y.replace(r'(^som.*)','Somashaker',regex=True, inplace = True)\n",
    "Y.replace(r'(^ch.*)', 'Chandrashekar_k', regex=True, inplace=True)\n",
    "Y.replace(r'(^us.*)', 'Usha', regex=True, inplace=True)\n",
    "Y.replace(r'(^bh.*)', 'Bhagavan', regex=True, inplace=True)\n",
    "Y.replace(r'(^sri.*)', 'Srinatha', regex=True, inplace=True)\n",
    "Y.replace(r'(^Na.*)', 'NaDisoza', regex=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_data[\"y_train\"] = Y\n",
    "vectors = train_data[\"features\"]\n",
    "arr = [0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "for i in vectors:\n",
    "    for k in range(2976-len(i)):\n",
    "        i.append(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = np.array(vectors)\n",
    "# vec\n",
    "size = []\n",
    "for i in vectors:\n",
    "        size.append(len(i))\n",
    "train_data[\"size\"] = size  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed Flat List 100\n"
     ]
    }
   ],
   "source": [
    "def flatten_list(_2d_list):\n",
    "    flat_list = []\n",
    "    # Iterate through the outer list\n",
    "    for element in _2d_list:\n",
    "        if type(element) is list:\n",
    "            # If the element is of type list, iterate through the sublist\n",
    "            for item in element:\n",
    "                flat_list.append(item)\n",
    "        else:\n",
    "            flat_list.append(element)\n",
    "    return flat_list\n",
    "\n",
    "flatten_final=[]\n",
    "for i in range(len(vectors)):\n",
    "    flatten_final.append(flatten_list(vec[i]))\n",
    "\n",
    "# nested_list = vectors[0]\n",
    "# print('Original List', len(nested_list)\n",
    "print('Transformed Flat List', len(flatten_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten_final = np.asarray(flatten_final, )\n",
    "a = flatten_final\n",
    "col_mean = np.nanmean(a, axis=0 )\n",
    "#Find indices that you need to replace\n",
    "inds = np.where(np.isnan(a))\n",
    "#Place column means in the indices. Align the arrays using take\n",
    "a[inds] = np.take(col_mean, inds[1])\n",
    "flatten_final = a\n",
    "flatten_final=sparse.csr_matrix(flatten_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "yy=train_data[\"y_train\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(flatten_final,yy,test_size=0.25, random_state=789)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Result\n",
      "Taining Accuracy : 1.0\n",
      "Testing Accuracy:  0.6\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "       Bhagavan       0.50      0.50      0.50         2\n",
      "Chandrashekar_k       0.40      0.50      0.44         4\n",
      "   Hrudayashiva       0.75      1.00      0.86         3\n",
      "       NaDisoza       0.80      1.00      0.89         4\n",
      "   Ravibeligeri       0.50      0.20      0.29         5\n",
      "     Somashaker       0.50      0.50      0.50         4\n",
      "       Srinatha       1.00      0.50      0.67         2\n",
      "           Usha       0.50      1.00      0.67         1\n",
      "\n",
      "    avg / total       0.60      0.60      0.57        25\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "cmodel_SVM =SVC(kernel='linear')\n",
    "cmodel_SVM = cmodel_SVM.fit(X_train, y_train)\n",
    "\n",
    "print(\"SVM Result\")\n",
    "print(\"Taining Accuracy :\",cmodel_SVM.score(X_train, y_train))\n",
    "print(\"Testing Accuracy: \",cmodel_SVM.score(X_test, y_test))\n",
    "\n",
    "predictions = cmodel_SVM.predict(X_test)\n",
    "print(classification_report(y_test, predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianNB(priors=None)\n",
      "Taining Accuracy : 0.49333333333333335\n",
      "Testing Accuracy:  0.36\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "       Bhagavan       0.20      0.50      0.29         2\n",
      "Chandrashekar_k       0.33      0.50      0.40         4\n",
      "   Hrudayashiva       0.43      1.00      0.60         3\n",
      "       NaDisoza       0.00      0.00      0.00         4\n",
      "   Ravibeligeri       0.00      0.00      0.00         5\n",
      "     Somashaker       0.40      0.50      0.44         4\n",
      "       Srinatha       0.50      0.50      0.50         2\n",
      "           Usha       0.00      0.00      0.00         1\n",
      "\n",
      "    avg / total       0.22      0.36      0.27        25\n",
      "\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "Taining Accuracy : 1.0\n",
      "Testing Accuracy:  0.4\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "       Bhagavan       0.00      0.00      0.00         2\n",
      "Chandrashekar_k       0.33      0.75      0.46         4\n",
      "   Hrudayashiva       0.33      0.67      0.44         3\n",
      "       NaDisoza       1.00      1.00      1.00         4\n",
      "   Ravibeligeri       0.00      0.00      0.00         5\n",
      "     Somashaker       0.00      0.00      0.00         4\n",
      "       Srinatha       0.00      0.00      0.00         2\n",
      "           Usha       0.50      1.00      0.67         1\n",
      "\n",
      "    avg / total       0.27      0.40      0.31        25\n",
      "\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=2, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=45, verbose=0, warm_start=False)\n",
      "Taining Accuracy : 0.6533333333333333\n",
      "Testing Accuracy:  0.32\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "       Bhagavan       0.00      0.00      0.00         2\n",
      "Chandrashekar_k       0.00      0.00      0.00         4\n",
      "   Hrudayashiva       0.14      0.33      0.20         3\n",
      "       NaDisoza       0.40      1.00      0.57         4\n",
      "   Ravibeligeri       0.33      0.20      0.25         5\n",
      "     Somashaker       0.67      0.50      0.57         4\n",
      "       Srinatha       0.00      0.00      0.00         2\n",
      "           Usha       0.00      0.00      0.00         1\n",
      "\n",
      "    avg / total       0.25      0.32      0.26        25\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def fun_model(model):\n",
    "    print(model)\n",
    "    model = model.fit(X_train.toarray(), y_train)\n",
    "    print(\"Taining Accuracy :\",model.score(X_train.toarray(), y_train))\n",
    "    print(\"Testing Accuracy: \",model.score(X_test.toarray(), y_test))\n",
    "    predictions = model.predict(X_test.toarray())\n",
    "    print(classification_report(y_test, predictions))\n",
    "\n",
    "    \n",
    "cmodel_NB = GaussianNB(priors=None)\n",
    "cmodel_Deci  = DecisionTreeClassifier()\n",
    "cmodel_Rand = RandomForestClassifier(max_depth=2, random_state=45)\n",
    "cmodel = [ cmodel_NB,cmodel_Deci ,cmodel_Rand]\n",
    "for i in cmodel:\n",
    "    fun_model(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Avg_wordLength(str):\n",
    "    norm=[]\n",
    "    str.translate(string.punctuation)\n",
    "    tokens = word_tokenize(str)\n",
    "    st = [\",\", \".\", \"'\", \"!\", '\"', \"#\", \"$\", \"%\", \"&\", \"(\", \")\", \"*\", \"+\", \"-\", \".\", \"/\", \":\", \";\", \"<\", \"=\", '>', \"?\",\n",
    "          \"@\", \"[\", \"\\\\\", \"]\", \"^\", \"_\", '`', \"{\", \"|\", \"}\", '~', '\\t', '\\n']\n",
    "    words = [word for word in tokens if word not in stopword and word not in st]\n",
    "    print(words)\n",
    "    for i in words: \n",
    "        print(len(i), end=\"           \")\n",
    "    return np.average([len(word) for word in words])\n",
    "\n",
    "s=\"ಹೋಳಿ ಹಬ್ಬದ ಸಂಭ್ರಮ,ಖುಷಿಗಳನ್ನು ನೀವೂ ಅನುಭವಿಸಿರುತ್ತೀರಿ.\"\n",
    "Avg_wordLength(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_all = [\"Avg_wordLength\",\"Avg_SentLenghtByCh\",\"Avg_SentLenghtByWord\",\"CountSpecialCharacter\",\"CountPuncuation\",\"typeTokenRatio\",\n",
    "         \n",
    "         \"hapaxLegemena\",\"BrunetsMeasureW\",\"YulesCharacteristicK\",\"ShannonEntropy\",\"SimpsonsIndex\",\"AvgWordFrequencyClass\",\"hapaxDisLegemena\"\n",
    "         ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs_all_short = [\"AwL\",\"ASLBC\",\"ASLBW\",\"CSC\",\"CP\",\"ttR\",\n",
    "         \n",
    "         \"HL\",\"BM\",\"YC\",\"SE\",\"SI\",\"AWFC\",\"HD\"\n",
    "         ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in fs_all:\n",
    "    print(i, end=\" \")\n",
    "print(\"\\n ------------------------------------------------------------------------------------------------------------------------\")\n",
    "for i in fs_all_short:\n",
    "    print(i, end=\"       \")\n",
    "# print(\"\\n\\n\")\n",
    "c= 0\n",
    "for i in avgValues.values():\n",
    "    c+=1\n",
    "    print(\"instance\", c)\n",
    "    for j in i:\n",
    "        print(\"{:.2f}\".format(j), end=\"    \" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
