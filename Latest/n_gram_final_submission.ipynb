{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# instance based Authorship attribution using n-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk import word_tokenize\n",
    "from pip._internal.utils.misc import tabulate\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import glob\n",
    "import pandas as pd\n",
    "#%run stylometric_Features.ipynb import FeatureExtration\n",
    "import seaborn as sns\n",
    "import matplotlib as plt\n",
    "import warnings\n",
    "import string\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.svm import SVC\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import tree\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "all_instances={\n",
    "'hrudayashiva':[\"01\",\"02\",\"03\",\"04\",\"05\",\"06\",\"07\",\"08\",\"09\",10,11,12],\n",
    "    'ravibeliger':[13,14,15,16,17,18,19,20,21,22,23,24,25],\n",
    "    'somashaker':[26,27,28,29,30,31,32,33,34,35,36],\n",
    "    'chandrashekark':[37,38,39,40,41,42,43,44,45,46,47],\n",
    "    'usha':[48,49,50,51,52,53,54,55,56,57,58,59,60],\n",
    "    'bhagavan':[61,62,63,64,65,66,67,68,69,70,71,72],\n",
    "    'NaDisoza':[73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92],\n",
    "    'srinatha':[93,94,95,96,97,98,99,100]\n",
    "}\n",
    "\n",
    "\n",
    "stop_words_list = \"C://Users/RAVIKUMAR/PycharmProjects/Authorship_Attribution_Instance/Data_2/new_stop_words.txt\"\n",
    "data_folder = \"C://Users/RAVIKUMAR/PycharmProjects/Authorship_Attribution_Instance/Data_2/train\"\n",
    "instance_by_author={}\n",
    "for author,AllFiles_Per_author in all_instances.items():\n",
    "    for i in AllFiles_Per_author:\n",
    "        for name in glob.glob(f\"C://Users/RAVIKUMAR/PycharmProjects/Authorship_Attribution_Instance/Data_2/train/instance{i}.txt\"):\n",
    "            with open(name, encoding='utf-8') as file:\n",
    "                instance_by_author[author+str(i)]=file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatiser = WordNetLemmatizer()\n",
    "stopword=[]\n",
    "\n",
    "with open(stop_words_list, encoding='utf-8') as file:\n",
    "    reader=file.read()\n",
    "    reader = [reader.split()]\n",
    "    stopword = sum(reader, [])\n",
    "\n",
    "pp = '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~0123456789'  \n",
    "def text_process(text):\n",
    "    nopunct = \"\".join([char for char in text if char not in pp])\n",
    "    tokens = nltk.word_tokenize(nopunct) \n",
    "    nopunct = \" \".join([word for word in tokens if word not in stopword])\n",
    "    return nopunct\n",
    "\n",
    "\n",
    "all_authors=[]\n",
    "Clean_Data=[]\n",
    "raw_data = []\n",
    "for author, file in instance_by_author.items():\n",
    "    all_authors.append(author)\n",
    "    raw_data.append(file)\n",
    "    Clean_Data.append(text_process(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame()\n",
    "df['author']=all_authors\n",
    "\n",
    "Y = pd.Series(df[\"author\"])\n",
    "Y.replace(r'(^ra.*)','Ravibeligeri',regex=True, inplace = True)\n",
    "Y.replace(r'(^hr.*)','Hrudayashiva',regex=True, inplace = True)\n",
    "Y.replace(r'(^som.*)','Somashaker',regex=True, inplace = True)\n",
    "Y.replace(r'(^ch.*)', 'Chandrashekar_k', regex=True, inplace=True)\n",
    "Y.replace(r'(^us.*)', 'Usha', regex=True, inplace=True)\n",
    "Y.replace(r'(^bh.*)', 'Bhagavan', regex=True, inplace=True)\n",
    "Y.replace(r'(^Na.*)', 'NaDisoza', regex=True, inplace=True)\n",
    "Y.replace(r'(^sri.*)', 'Srinatha', regex=True, inplace=True)\n",
    "df['Clean_Data']=Clean_Data\n",
    "df[\"raw_data\"] = raw_data\n",
    "y = Y\n",
    "X = df['Clean_Data']\n",
    "\n",
    "# 80-20 splitting the dataset (80%->Training and 20%->Validation)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.35, random_state=6789)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_all(n_value_vector):  \n",
    "    word_vector_train = n_value_vector.transform(X_train)\n",
    "    word_vector_test = n_value_vector.transform(X_test)\n",
    "    \n",
    "    model1 =SVC(kernel='linear')\n",
    "    model1 = model1.fit(word_vector_train, y_train)\n",
    "\n",
    "\n",
    "    modelNB = GaussianNB()\n",
    "    modelNB = modelNB.fit(word_vector_train.toarray(), y_train)\n",
    "\n",
    "    model_Deci  = tree.DecisionTreeClassifier()\n",
    "    model_Deci = model_Deci.fit(word_vector_train.toarray(), y_train)\n",
    "\n",
    "    model_Rand = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "    model_Rand = model_Rand.fit(word_vector_train, y_train)\n",
    "    \n",
    "  \n",
    "    print(\"SVM Training Accuracy        :\",model1.score(word_vector_train, y_train))\n",
    "    print(\"SVM Testing Acuuracy         : \",model1.score(word_vector_test, y_test))\n",
    "    print()\n",
    "    print(\"modelNB Training Accuracy    :\",modelNB.score(word_vector_train.toarray(), y_train))\n",
    "    print(\"modelNB Testing Acuuracy     : \",modelNB.score(word_vector_test.toarray(), y_test))\n",
    "    print()\n",
    "    print(\"model_Deci Training Accuracy :\",model_Deci.score(word_vector_train.toarray(), y_train))\n",
    "    print(\"model_Deci Testing Acuuracy  : \",model_Deci.score(word_vector_test.toarray(), y_test))\n",
    "    print()\n",
    "    print(\"model_Rand Training Accuracy :\",model_Rand.score(word_vector_train, y_train))\n",
    "    print(\"model_Rand Testing Acuuracy  : \",model_Rand.score(word_vector_test, y_test))\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word and Char n-gram with n = 1,2,3,4,5,6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# n = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------word n gram----------- \n",
      "SVM Training Accuracy        : 1.0\n",
      "SVM Testing Acuuracy         :  0.8\n",
      "\n",
      "modelNB Training Accuracy    : 1.0\n",
      "modelNB Testing Acuuracy     :  0.5428571428571428\n",
      "\n",
      "model_Deci Training Accuracy : 1.0\n",
      "model_Deci Testing Acuuracy  :  0.6285714285714286\n",
      "\n",
      "model_Rand Training Accuracy : 0.7384615384615385\n",
      "model_Rand Testing Acuuracy  :  0.5428571428571428\n",
      "*************************************************************************************************\n",
      "---------------char n gram----------- \n",
      "SVM Training Accuracy        : 0.6\n",
      "SVM Testing Acuuracy         :  0.6\n",
      "\n",
      "modelNB Training Accuracy    : 0.9846153846153847\n",
      "modelNB Testing Acuuracy     :  0.8571428571428571\n",
      "\n",
      "model_Deci Training Accuracy : 1.0\n",
      "model_Deci Testing Acuuracy  :  0.9428571428571428\n",
      "\n",
      "model_Rand Training Accuracy : 0.7692307692307693\n",
      "model_Rand Testing Acuuracy  :  0.6857142857142857\n"
     ]
    }
   ],
   "source": [
    "print(\"-------------word n gram----------- \")\n",
    "one_word=TfidfVectorizer(analyzer=\"word\", ngram_range=(1,1),max_features=2000, binary=False).fit(X_train)\n",
    "model_all(one_word)\n",
    "print(\"*************************************************************************************************\")\n",
    "print(\"---------------char n gram----------- \")\n",
    "one_char=TfidfVectorizer(analyzer=\"char\", ngram_range=(1,1),max_features=2000, binary=False).fit(X_train)\n",
    "model_all(one_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# n = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------word n gram----------- \n",
      "SVM Training Accuracy        : 1.0\n",
      "SVM Testing Acuuracy         :  0.8285714285714286\n",
      "\n",
      "modelNB Training Accuracy    : 1.0\n",
      "modelNB Testing Acuuracy     :  0.5142857142857142\n",
      "\n",
      "model_Deci Training Accuracy : 1.0\n",
      "model_Deci Testing Acuuracy  :  0.6285714285714286\n",
      "\n",
      "model_Rand Training Accuracy : 0.6615384615384615\n",
      "model_Rand Testing Acuuracy  :  0.4\n",
      "*************************************************************************************************\n",
      "---------------char n gram----------- \n",
      "SVM Training Accuracy        : 0.6923076923076923\n",
      "SVM Testing Acuuracy         :  0.6\n",
      "\n",
      "modelNB Training Accuracy    : 1.0\n",
      "modelNB Testing Acuuracy     :  0.6285714285714286\n",
      "\n",
      "model_Deci Training Accuracy : 1.0\n",
      "model_Deci Testing Acuuracy  :  0.5714285714285714\n",
      "\n",
      "model_Rand Training Accuracy : 0.8769230769230769\n",
      "model_Rand Testing Acuuracy  :  0.7428571428571429\n"
     ]
    }
   ],
   "source": [
    "print(\"-------------word n gram----------- \")\n",
    "two_word=TfidfVectorizer(analyzer=\"word\", ngram_range=(2,2),max_features=2000, binary=False).fit(X_train)\n",
    "\n",
    "model_all(two_word)\n",
    "print(\"*************************************************************************************************\")\n",
    "print(\"---------------char n gram----------- \")\n",
    "two_char=TfidfVectorizer(analyzer=\"char\", ngram_range=(2,2),max_features=2000, binary=False).fit(X_train)\n",
    "model_all(two_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# n = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------word n gram----------- \n",
      "SVM Training Accuracy        : 1.0\n",
      "SVM Testing Acuuracy         :  0.6285714285714286\n",
      "\n",
      "modelNB Training Accuracy    : 1.0\n",
      "modelNB Testing Acuuracy     :  0.6857142857142857\n",
      "\n",
      "model_Deci Training Accuracy : 1.0\n",
      "model_Deci Testing Acuuracy  :  0.45714285714285713\n",
      "\n",
      "model_Rand Training Accuracy : 0.5692307692307692\n",
      "model_Rand Testing Acuuracy  :  0.3142857142857143\n",
      "*************************************************************************************************\n",
      "---------------char n gram----------- \n",
      "SVM Training Accuracy        : 0.9846153846153847\n",
      "SVM Testing Acuuracy         :  0.8285714285714286\n",
      "\n",
      "modelNB Training Accuracy    : 1.0\n",
      "modelNB Testing Acuuracy     :  0.5428571428571428\n",
      "\n",
      "model_Deci Training Accuracy : 1.0\n",
      "model_Deci Testing Acuuracy  :  0.6\n",
      "\n",
      "model_Rand Training Accuracy : 0.9230769230769231\n",
      "model_Rand Testing Acuuracy  :  0.6857142857142857\n"
     ]
    }
   ],
   "source": [
    "print(\"-------------word n gram----------- \")\n",
    "three_word=TfidfVectorizer(analyzer=\"word\", ngram_range=(3,3),max_features=2000, binary=False).fit(X_train)\n",
    "model_all(three_word)\n",
    "print(\"*************************************************************************************************\")\n",
    "print(\"---------------char n gram----------- \")\n",
    "three_char=TfidfVectorizer(analyzer=\"char\", ngram_range=(3,3),max_features=2000, binary=False).fit(X_train)\n",
    "model_all(three_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# n = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------word n gram----------- \n",
      "SVM Training Accuracy        : 0.9692307692307692\n",
      "SVM Testing Acuuracy         :  0.3142857142857143\n",
      "\n",
      "modelNB Training Accuracy    : 0.9846153846153847\n",
      "modelNB Testing Acuuracy     :  0.37142857142857144\n",
      "\n",
      "model_Deci Training Accuracy : 0.9846153846153847\n",
      "model_Deci Testing Acuuracy  :  0.17142857142857143\n",
      "\n",
      "model_Rand Training Accuracy : 0.49230769230769234\n",
      "model_Rand Testing Acuuracy  :  0.11428571428571428\n",
      "*************************************************************************************************\n",
      "---------------char n gram----------- \n",
      "SVM Training Accuracy        : 1.0\n",
      "SVM Testing Acuuracy         :  0.9142857142857143\n",
      "\n",
      "modelNB Training Accuracy    : 1.0\n",
      "modelNB Testing Acuuracy     :  0.5428571428571428\n",
      "\n",
      "model_Deci Training Accuracy : 1.0\n",
      "model_Deci Testing Acuuracy  :  0.6857142857142857\n",
      "\n",
      "model_Rand Training Accuracy : 0.8461538461538461\n",
      "model_Rand Testing Acuuracy  :  0.7142857142857143\n"
     ]
    }
   ],
   "source": [
    "print(\"-------------word n gram----------- \")\n",
    "four_word=TfidfVectorizer(analyzer=\"word\", ngram_range=(4,4),max_features=2000, binary=False).fit(X_train)\n",
    "model_all(four_word)\n",
    "print(\"*************************************************************************************************\")\n",
    "print(\"---------------char n gram----------- \")\n",
    "four_char=TfidfVectorizer(analyzer=\"char\", ngram_range=(4,4),max_features=2000, binary=False).fit(X_train)\n",
    "model_all(four_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# n = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------word n gram----------- \n",
      "SVM Training Accuracy        : 0.9692307692307692\n",
      "SVM Testing Acuuracy         :  0.14285714285714285\n",
      "\n",
      "modelNB Training Accuracy    : 0.9692307692307692\n",
      "modelNB Testing Acuuracy     :  0.14285714285714285\n",
      "\n",
      "model_Deci Training Accuracy : 0.9846153846153847\n",
      "model_Deci Testing Acuuracy  :  0.14285714285714285\n",
      "\n",
      "model_Rand Training Accuracy : 0.4307692307692308\n",
      "model_Rand Testing Acuuracy  :  0.14285714285714285\n",
      "*************************************************************************************************\n",
      "---------------char n gram----------- \n",
      "SVM Training Accuracy        : 1.0\n",
      "SVM Testing Acuuracy         :  0.9428571428571428\n",
      "\n",
      "modelNB Training Accuracy    : 1.0\n",
      "modelNB Testing Acuuracy     :  0.5714285714285714\n",
      "\n",
      "model_Deci Training Accuracy : 1.0\n",
      "model_Deci Testing Acuuracy  :  0.8\n",
      "\n",
      "model_Rand Training Accuracy : 0.8153846153846154\n",
      "model_Rand Testing Acuuracy  :  0.6571428571428571\n"
     ]
    }
   ],
   "source": [
    "print(\"-------------word n gram----------- \")\n",
    "five_word=TfidfVectorizer(analyzer=\"word\", ngram_range=(5,5),max_features=2000, binary=False).fit(X_train)\n",
    "model_all(five_word)\n",
    "print(\"*************************************************************************************************\")\n",
    "print(\"---------------char n gram----------- \")\n",
    "five_char=TfidfVectorizer(analyzer=\"char\", ngram_range=(5,5),max_features=2000, binary=False).fit(X_train)\n",
    "model_all(five_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# n = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------word n gram----------- \n",
      "SVM Training Accuracy        : 0.9538461538461539\n",
      "SVM Testing Acuuracy         :  0.11428571428571428\n",
      "\n",
      "modelNB Training Accuracy    : 0.9538461538461539\n",
      "modelNB Testing Acuuracy     :  0.14285714285714285\n",
      "\n",
      "model_Deci Training Accuracy : 0.9846153846153847\n",
      "model_Deci Testing Acuuracy  :  0.11428571428571428\n",
      "\n",
      "model_Rand Training Accuracy : 0.4153846153846154\n",
      "model_Rand Testing Acuuracy  :  0.14285714285714285\n",
      "*************************************************************************************************\n",
      "---------------char n gram----------- \n",
      "SVM Training Accuracy        : 1.0\n",
      "SVM Testing Acuuracy         :  0.9428571428571428\n",
      "\n",
      "modelNB Training Accuracy    : 1.0\n",
      "modelNB Testing Acuuracy     :  0.6\n",
      "\n",
      "model_Deci Training Accuracy : 1.0\n",
      "model_Deci Testing Acuuracy  :  0.8\n",
      "\n",
      "model_Rand Training Accuracy : 0.8461538461538461\n",
      "model_Rand Testing Acuuracy  :  0.8285714285714286\n"
     ]
    }
   ],
   "source": [
    "print(\"-------------word n gram----------- \")\n",
    "six_word=TfidfVectorizer(analyzer=\"word\", ngram_range=(6,6),max_features=2000, binary=False).fit(X_train)\n",
    "model_all(six_word)\n",
    "print(\"*************************************************************************************************\")\n",
    "print(\"---------------char n gram----------- \")\n",
    "six_char=TfidfVectorizer(analyzer=\"char\", ngram_range=(6,6),max_features=2000, binary=False).fit(X_train)\n",
    "model_all(six_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calling each tf-idf vectors and union\n",
    "UnionFeater = FeatureUnion([                        \n",
    "                        (\"word_1\",one_word),(\"word_2\",two_word),(\"word_3\",three_word),(\"word_4\",four_word),\n",
    "                           (\"word_5\",five_word),\n",
    "     (\"char_1\",one_char),(\"char_2\",two_char),(\"char_3\",three_char),(\"char_4\",four_char),\n",
    "                           (\"char_5\",five_char)\n",
    "                           ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Training Accuracy        : 1.0\n",
      "SVM Testing Acuuracy         :  0.9428571428571428\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "       Bhagavan       1.00      1.00      1.00         3\n",
      "Chandrashekar_k       1.00      1.00      1.00         3\n",
      "   Hrudayashiva       0.75      1.00      0.86         3\n",
      "       NaDisoza       1.00      1.00      1.00        11\n",
      "   Ravibeligeri       1.00      0.75      0.86         4\n",
      "     Somashaker       1.00      0.75      0.86         4\n",
      "       Srinatha       1.00      1.00      1.00         3\n",
      "           Usha       0.80      1.00      0.89         4\n",
      "\n",
      "    avg / total       0.96      0.94      0.94        35\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#TF-IDF vectorizer\n",
    "vector_train = UnionFeater.transform(X_train)\n",
    "vector_test = UnionFeater.transform(X_test)\n",
    "\n",
    "SVM =SVC(kernel='linear')\n",
    "SVM = SVM.fit(vector_train, y_train)\n",
    "\n",
    "print(\"SVM Training Accuracy        :\",SVM.score(vector_train, y_train))\n",
    "print(\"SVM Testing Acuuracy         : \",SVM.score(vector_test, y_test))\n",
    "predictions = SVM.predict(vector_test)\n",
    "print(classification_report(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model used:  GaussianNB(priors=None)\n",
      " Training Accuracy        : 1.0\n",
      " Testing Acuuracy         :  0.6571428571428571\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "       Bhagavan       0.33      1.00      0.50         3\n",
      "Chandrashekar_k       1.00      0.33      0.50         3\n",
      "   Hrudayashiva       0.50      0.33      0.40         3\n",
      "       NaDisoza       0.85      1.00      0.92        11\n",
      "   Ravibeligeri       0.00      0.00      0.00         4\n",
      "     Somashaker       0.00      0.00      0.00         4\n",
      "       Srinatha       0.75      1.00      0.86         3\n",
      "           Usha       0.67      1.00      0.80         4\n",
      "\n",
      "    avg / total       0.56      0.66      0.57        35\n",
      "\n",
      "Model used:  DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      " Training Accuracy        : 1.0\n",
      " Testing Acuuracy         :  0.7142857142857143\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "       Bhagavan       0.00      0.00      0.00         3\n",
      "Chandrashekar_k       1.00      1.00      1.00         3\n",
      "   Hrudayashiva       0.50      0.33      0.40         3\n",
      "       NaDisoza       1.00      0.91      0.95        11\n",
      "   Ravibeligeri       1.00      0.25      0.40         4\n",
      "     Somashaker       0.43      0.75      0.55         4\n",
      "       Srinatha       0.60      1.00      0.75         3\n",
      "           Usha       0.57      1.00      0.73         4\n",
      "\n",
      "    avg / total       0.72      0.71      0.67        35\n",
      "\n",
      "Model used:  RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=2, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=0, verbose=0, warm_start=False)\n",
      " Training Accuracy        : 0.9076923076923077\n",
      " Testing Acuuracy         :  0.6857142857142857\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "       Bhagavan       0.00      0.00      0.00         3\n",
      "Chandrashekar_k       1.00      0.33      0.50         3\n",
      "   Hrudayashiva       0.50      1.00      0.67         3\n",
      "       NaDisoza       0.85      1.00      0.92        11\n",
      "   Ravibeligeri       1.00      0.75      0.86         4\n",
      "     Somashaker       0.00      0.00      0.00         4\n",
      "       Srinatha       0.67      0.67      0.67         3\n",
      "           Usha       0.57      1.00      0.73         4\n",
      "\n",
      "    avg / total       0.63      0.69      0.63        35\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cmodel_SVM = SVC(kernel='linear', C=1, random_state=42)\n",
    "cmodel_NB = GaussianNB()\n",
    "cmodel_Deci  = tree.DecisionTreeClassifier()\n",
    "cmodel_Rand = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "cmodel = [ cmodel_NB,cmodel_Deci ,cmodel_Rand]\n",
    "\n",
    "def allModel(model):\n",
    "    vector_train = UnionFeater.transform(X_train)\n",
    "    vector_test = UnionFeater.transform(X_test)\n",
    "    model = model.fit(vector_train.toarray(), y_train)\n",
    "    print(\" Training Accuracy        :\",model.score(vector_train.toarray(), y_train))\n",
    "    print(\" Testing Acuuracy         : \",model.score(vector_test.toarray(), y_test))\n",
    "    predictions = model.predict(vector_test.toarray())\n",
    "    print(classification_report(y_test, predictions))\n",
    "for i in cmodel:\n",
    "    print(\"Model used: \", i)\n",
    "    allModel(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cross Validation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross1=TfidfVectorizer(analyzer=\"word\", ngram_range=(1,5),max_features=2000, binary=False).fit(X)\n",
    "cross2=TfidfVectorizer(analyzer=\"char\", ngram_range=(1,5),max_features=2000, binary=False).fit(X)\n",
    "# cross_vecotr1 = cross1.transform(X)\n",
    "# cross_vecotr2 = cross2.transform(X)\n",
    "\n",
    "\n",
    "\n",
    "crossfeatures = FeatureUnion([\n",
    "                        \n",
    "                        (\"word\",cross1),(\"char\",cross2)\n",
    "                           \n",
    "] )\n",
    "cross_vecotr = crossfeatures.fit_transform(X)\n",
    "\n",
    "\n",
    "authors = y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmodel_SVM = SVC(kernel='linear', C=1, random_state=42)\n",
    "cmodel_NB = GaussianNB()\n",
    "cmodel_Deci  = tree.DecisionTreeClassifier()\n",
    "cmodel_Rand = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "cmodel = [cmodel_NB,cmodel_Deci ,cmodel_Rand]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM cross_validation with CV = 10\n",
      "train score:  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "test score:  [0.93333333 0.76923077 0.81818182 0.88888889 1.         0.88888889\n",
      " 1.         1.         0.625      0.875     ]\n"
     ]
    }
   ],
   "source": [
    "print(\"SVM cross_validation with CV = 10\")\n",
    "scores = cross_validate(cmodel_SVM, cross_vecotr, authors, cv=10)\n",
    "print(\"train score: \", scores[\"train_score\"])\n",
    "print(\"test score: \", scores[\"test_score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianNB(priors=None) {'fit_time': array([0.01799202, 0.0129962 , 0.01099849, 0.01099849, 0.01099539,\n",
      "       0.01099658, 0.00999761, 0.01199985, 0.01099801, 0.01101804]), 'score_time': array([0.00801253, 0.00499821, 0.00399303, 0.00699377, 0.00399709,\n",
      "       0.00400305, 0.0039978 , 0.00299501, 0.00299835, 0.00299764]), 'test_score': array([0.46666667, 0.46153846, 0.36363636, 0.33333333, 0.44444444,\n",
      "       0.66666667, 0.44444444, 0.55555556, 0.25      , 0.5       ]), 'train_score': array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])}\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best') {'fit_time': array([0.10297179, 0.11198449, 0.11195087, 0.11496091, 0.11494732,\n",
      "       0.11596465, 0.11096025, 0.10897899, 0.10694408, 0.10698438]), 'score_time': array([0.0010004 , 0.00100493, 0.00201178, 0.00100112, 0.00100064,\n",
      "       0.        , 0.        , 0.00099993, 0.0010004 , 0.00100017]), 'test_score': array([0.8       , 0.38461538, 0.81818182, 0.88888889, 0.88888889,\n",
      "       0.66666667, 1.        , 0.77777778, 0.75      , 0.75      ]), 'train_score': array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])}\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=2, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=0, verbose=0, warm_start=False) {'fit_time': array([0.03999066, 0.03800225, 0.03297281, 0.03498745, 0.0359695 ,\n",
      "       0.03699207, 0.03397322, 0.03700233, 0.03597164, 0.03798437]), 'score_time': array([0.00199938, 0.00199914, 0.0010004 , 0.00100017, 0.00300527,\n",
      "       0.00199938, 0.00199914, 0.00199962, 0.00199962, 0.00199628]), 'test_score': array([0.8       , 0.53846154, 0.63636364, 0.77777778, 0.55555556,\n",
      "       0.66666667, 0.77777778, 0.77777778, 0.625     , 0.75      ]), 'train_score': array([0.87058824, 0.85057471, 0.8988764 , 0.81318681, 0.82417582,\n",
      "       0.78021978, 0.8021978 , 0.81318681, 0.88043478, 0.88043478])}\n"
     ]
    }
   ],
   "source": [
    "for i in cmodel:\n",
    "    scores = cross_validate(i, cross_vecotr.toarray(), authors, cv=10)\n",
    "    print(i,scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# instance: each instance result -n gram with union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instance</th>\n",
       "      <th>vector:sparse matrix sum value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>ಕಾಡು ನಮ್ಮದಲ್ಲ ಒಳಹೋದಂತೆಲ್ಲ ದಟ್ಟವಾದ ಕಾಡು ಸೂರ್ಯನ ...</td>\n",
       "      <td>121.307195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>ಮುದ್ದು ಹುಡುಗಿ ಮನೋಹರಿ ಹಾಗು ರಘುರಾಮರಿಗೆ ಹಿಂದಿನಂತಾ...</td>\n",
       "      <td>113.145777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>ಅದಕ್ಕೆ ಯಾಕೆ ಹಾಗೆ ಹೆಸರಿಟ್ಟೆನೋ ಗೊತ್ತಿಲ್ಲ ಆಗಿನ್ನೂ...</td>\n",
       "      <td>103.622260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ಹೆಂಡತಿಯು ಹದ ಮಾಡಿಕೊಟ್ಟ ತಾಂಬೂಲ ಜಗಿಯುತ್ತಲೇ ರಾಜಾಜಿ...</td>\n",
       "      <td>109.012959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>ಇರುವುದೆಲ್ಲವ ಬಿಟ್ಟು ಇರದುದರೆಡೆಗೆ ತುಡಿವುದೇ ಜೀವನ ಎ...</td>\n",
       "      <td>85.767055</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             instance  \\\n",
       "72  ಕಾಡು ನಮ್ಮದಲ್ಲ ಒಳಹೋದಂತೆಲ್ಲ ದಟ್ಟವಾದ ಕಾಡು ಸೂರ್ಯನ ...   \n",
       "83  ಮುದ್ದು ಹುಡುಗಿ ಮನೋಹರಿ ಹಾಗು ರಘುರಾಮರಿಗೆ ಹಿಂದಿನಂತಾ...   \n",
       "21  ಅದಕ್ಕೆ ಯಾಕೆ ಹಾಗೆ ಹೆಸರಿಟ್ಟೆನೋ ಗೊತ್ತಿಲ್ಲ ಆಗಿನ್ನೂ...   \n",
       "3   ಹೆಂಡತಿಯು ಹದ ಮಾಡಿಕೊಟ್ಟ ತಾಂಬೂಲ ಜಗಿಯುತ್ತಲೇ ರಾಜಾಜಿ...   \n",
       "35  ಇರುವುದೆಲ್ಲವ ಬಿಟ್ಟು ಇರದುದರೆಡೆಗೆ ತುಡಿವುದೇ ಜೀವನ ಎ...   \n",
       "\n",
       "    vector:sparse matrix sum value  \n",
       "72                      121.307195  \n",
       "83                      113.145777  \n",
       "21                      103.622260  \n",
       "3                       109.012959  \n",
       "35                       85.767055  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eachInstanceResult  = pd.DataFrame()\n",
    "eachInstanceResult[\"instance\"]=X_train\n",
    "eachInstanceResult[\"vector:sparse matrix sum value\"]=vector_train.sum(axis=1)\n",
    "eachInstanceResult.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
